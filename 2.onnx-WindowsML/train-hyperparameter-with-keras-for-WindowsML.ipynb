{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "bf74d2e9-2708-49b1-934b-e0ede342f475"
        }
      },
      "cell_type": "markdown",
      "source": "# Training, hyperparameter tune with Keras and make ONNX file for WindowsML\n\n## Introduction\nThis tutorial shows how to train a simple deep neural network using the MNIST dataset and Keras on Azure Machine Learning. MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of `28x28` pixels, representing number from 0 to 9. The goal is to create a multi-class classifier to identify the digit each image represents, and deploy it as a web service in Azure.\n\nFor more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/).\n\n## Prerequisite:\n* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n    * install the AML SDK\n    * create a workspace and its configuration file (`config.json`)\n* For local scoring test, you will also need to have `tensorflow` and `keras` installed in the current Jupyter kernel."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 0. Preparation\n### Pre-Run\n\nCheck installed Pypi files."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#!pip freeze",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "if module in below is NOT installed, Run next cell once and then **restart kernel**."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#!pip install --upgrade scikit-image\n#!pip install --upgrade azureml-sdk[notebooks,automl,explain,contrib]\n#!pip install --upgrade onnxmltools\n\n# Note you will need to have extensions enabled prior to jupyter kernel starting\n#!jupyter nbextension install --py --sys-prefix azureml.contrib.explain.model.visualize\n#!jupyter nbextension enable --py --sys-prefix azureml.contrib.explain.model.visualize",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### SSO to Azure\nLet's DO this process first.\n\n**Menu [Azure] - [Connect to Azure] ** to enable Single Sign on to Azure services."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's get started. First let's import some Python libraries."
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "c377ea0c-0cd9-4345-9be2-e20fb29c94c3"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml\nfrom azureml.core import Workspace\n\n# check core SDK version number\nprint('This code run confrimed - SDK version: 1.0.48')\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "This code run confrimed - SDK version: 1.0.48\nAzure ML SDK Version:  1.0.57\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Initialize workspace\nInitialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config()\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Subscription id: ' + ws.subscription_id, \n      'Resource group: ' + ws.resource_group, sep='\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
        }
      },
      "cell_type": "markdown",
      "source": "### Create an Azure ML experiment\nLet's create an experiment named \"keras-mnist\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nexp = Experiment(workspace=ws, name='keras-mnist')",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "defe921f-8097-44c3-8336-8af6700804a7"
        }
      },
      "cell_type": "markdown",
      "source": "### Download MNIST dataset\nIn order to train on the MNIST dataset we will first need to download it from Yan LeCun's web site directly and save them in a `data` folder locally."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib\n\nos.makedirs('./data/mnist', exist_ok=True)\n\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename='./data/mnist/train-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename='./data/mnist/train-labels.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename='./data/mnist/test-images.gz')\nurllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename='./data/mnist/test-labels.gz')",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "('./data/mnist/test-labels.gz', <http.client.HTTPMessage at 0x7f2ef848a4a8>)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
        }
      },
      "cell_type": "markdown",
      "source": "### Load data and Show some sample images\nLet's load the downloaded compressed file into numpy arrays using some utility functions included in the `utils.py` library file from the current folder. Then we use `matplotlib` to plot 30 random images from the dataset along with their labels."
    },
    {
      "metadata": {
        "nbpresent": {
          "id": "396d478b-34aa-4afa-9898-cdce8222a516"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "from utils import load_data, one_hot_encode\n\n# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\nx_train = load_data('./data/mnist/train-images.gz', False) / 255.0\ny_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n\nx_test = load_data('./data/mnist/test-images.gz', False) / 255.0\ny_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)\n\ncount = 0\nsample_size = 30\nplt.figure(figsize = (16, 6))\nfor i in np.random.permutation(x_train.shape[0])[:sample_size]:\n    count = count + 1\n    plt.subplot(1, sample_size, count)\n    plt.axhline('')\n    plt.axvline('')\n    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n    plt.imshow(x_train[i].reshape(28, 28), cmap = plt.cm.Greys)\nplt.show()",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 1152x432 with 30 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Upload MNIST dataset to default datastore \nA [datastore](https://docs.microsoft.com/azure/machine-learning/service/how-to-access-data) is a place where data can be stored that is then made accessible to a Run either by means of mounting or copying the data to the compute target. A datastore can either be backed by an Azure Blob Storage or and Azure File Share (ADLS will be supported in the future). For simple data handling, each workspace provides a default datastore that can be used, in case the data is not already in Blob Storage or File Share."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this next step, we will upload the training and test set into the workspace's default datastore, which we will then later be mount on an `AmlCompute` cluster for training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds.upload(src_dir='./data/mnist', target_path='mnist', overwrite=True, show_progress=True)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Uploading an estimated of 4 files\nUploading ./data/mnist/test-images.gz\nUploading ./data/mnist/test-labels.gz\nUploading ./data/mnist/train-images.gz\nUploading ./data/mnist/train-labels.gz\nUploaded ./data/mnist/test-labels.gz, 1 files out of an estimated total of 4\nUploaded ./data/mnist/train-labels.gz, 2 files out of an estimated total of 4\nUploaded ./data/mnist/test-images.gz, 3 files out of an estimated total of 4\nUploaded ./data/mnist/train-images.gz, 4 files out of an estimated total of 4\nUploaded 4 files\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_da6bcbf05ee84964aa9dee91e53d9967"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get default AmlCompute\nYou can create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you use default `AmlCompute` as your training compute resource."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\ncompute_target = ws.get_default_compute_target(type=\"GPU\") #GPU\nprint(compute_target)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "AmlCompute(workspace=Workspace.create(name='dahatakemlus', subscription_id='ca28e367-312d-4b07-a3fd-650390469a4e', resource_group='dahatakemlus'), name=gpu-cluster, id=/subscriptions/ca28e367-312d-4b07-a3fd-650390469a4e/resourceGroups/dahatakemlus/providers/Microsoft.MachineLearningServices/workspaces/dahatakemlus/computes/gpu-cluster, type=AmlCompute, provisioning_state=Succeeded, location=westus2, tags=None)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Calcurate Count of GPU\n\nThis is for useful **multi GPU** senario like NC12,24 series.\n\nvm_size list\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-gpu"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json\nimport re\n\ncomp_dict = compute_target.get_status().serialize()\nvm_size = comp_dict['vmSize']\nprint(vm_size)\n\ndef get_gpu_count(vm_size):\n    pattern=r'\\d{1,2}'\n    s = re.search(pattern, vm_size)\n    gpu_count = vm_size[s.start():s.end()]\n    return int(gpu_count) // 6\n\ndef tensorcore_enabled(vm_size):\n    result = 0\n    strs = vm_size.split('_')\n    v = strs[2].replace(\"V\",\"\")\n    if int(v) >= 2:\n        result = 1\n    \n    return result\n\ngpu_count = get_gpu_count(vm_size)\nprint('gpu_count:' + str(gpu_count))\n\ntensorcore_status = tensorcore_enabled(vm_size)\nprint('tensorcore_enabled:' + str(tensorcore_status))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "STANDARD_NC6S_V3\ngpu_count:1\ntensorcore_enabled:1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Copy the training files into the script folder\nThe Keras training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import shutil\n\nscript_folder = './keras-mnist'\nos.makedirs(script_folder, exist_ok=True)\n\n# the training logic is in the keras_mnist.py file.\nshutil.copy('./mnist_cnn.py', script_folder)\n\n# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\nshutil.copy('./utils.py', script_folder)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "'./keras-mnist/utils.py'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Azure ML concepts  \nPlease note the following three things in the code below:\n1. The script accepts arguments using the argparse package. In this case there is one argument `--data_folder` which specifies the file system folder in which the script can find the MNIST data\n```\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_folder')\n```\n2. The script is accessing the Azure ML `Run` object by executing `run = Run.get_context()`. Further down the script is using the `run` to report the loss and accuracy at the end of each epoch via callback.\n```\n    run.log('Loss', log['loss'])\n    run.log('Accuracy', log['acc'])\n```\n3. When running the script on Azure ML, you can write files out to a folder `./outputs` that is relative to the root directory. This folder is specially tracked by Azure ML in the sense that any files written to that folder during script execution on the remote target will be picked up by Run History; these files (known as artifacts) will be available as part of the run history record."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next cell will print out the training code for you to inspect."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#with open(os.path.join(script_folder, './keras_mnist.py'), 'r') as f:\nwith open(os.path.join(script_folder, './mnist_cnn.py'), 'r') as f:\n    print(f.read())",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License.\nfrom __future__ import print_function\n\nimport numpy as np\nimport argparse\nimport os\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import Callback\nfrom keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\nfrom keras.utils import multi_gpu_model\nfrom keras import backend as K\n\nfrom azureml.core import Run\nfrom utils import load_data, one_hot_encode\n\n# parse parameters\noptimizer_types = {\n    'SGD': lambda lr: SGD(lr=lr),\n    'RMSprop': lambda lr: RMSprop(lr=lr),\n    'Adagrad': lambda lr: Adagrad(lr=lr),\n    'Adadelta': lambda lr: Adadelta(lr=lr),\n    'Adam': lambda lr: Adam(lr=lr),\n    'Adamax': lambda lr: Adamax(lr=lr),\n    'Nadam': lambda lr: Nadam(lr=lr)\n}\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nparser.add_argument('--batch-size', type=int, dest='batch_size', default=50, help='mini batch size for training')\nparser.add_argument('--epoch', type=int, dest='epoch', default=20, help='epoch size for training')\nparser.add_argument('--neurons-1', type=int, dest='neurons_1', default=32, help='# of neurons in the first layer')\nparser.add_argument('--neurons-2', type=int, dest='neurons_2', default=64,help='# of neurons in the second layer')\nparser.add_argument('--neurons-3', type=int, dest='neurons_3', default=128, help='# of neurons in the third layer')\nparser.add_argument('--kernel-size-1', type=int, dest='kernel_size_1', default=3, help='kernel size of first layer')\nparser.add_argument('--kernel-size-2', type=int, dest='kernel_size_2', default=3, help='kernel size of second layer')\nparser.add_argument('--pool-size', type=int, dest='pool_size', default=2, help='# of neurons in the third layer')                    \nparser.add_argument('--learning-rate', type=float, dest='learning_rate', default=0.001, help='learning rate')\nparser.add_argument('--activation', type=str, dest='activation', default='relu', help='activation function')\nparser.add_argument('--optimizer', type=str, dest='optimizer', default='RMSprop', help='Optimzers to use for training. Defaults to RMSProp for initial training and SGD for subsequent.')\nparser.add_argument('--loss', type=str, dest='loss', default='categorical_crossentropy', help='loss function.')\nparser.add_argument('--dropout-1', type=float, dest='dropout_1', default=0.25, help='Drop Out rate 1st')\nparser.add_argument('--dropout-2', type=float, dest='dropout_2', default=0.5, help='Drop Out rate 2nd')\nparser.add_argument('--gpu', type=int, dest='gpu', default=1, help='The count of GPU')\nparser.add_argument('--auto_mixed_precision', type=int, dest='auto_mixed_precision', default=1, help='Enable Automatic Mixed Precision to use Tensor Core')\n\nargs = parser.parse_args()\n\ndata_folder = args.data_folder\n\nimg_rows, img_cols = 28, 28\nn_classes = 10\nneurons_1 = args.neurons_1\nneurons_2 = args.neurons_2\nneurons_3 = args.neurons_3\nkernel_size_1 = args.kernel_size_1\nkernel_size_2 = args.kernel_size_2\npool_size = args.pool_size\ndropout_1 = args.dropout_1\ndropout_2 = args.dropout_2\n\nepochs = args.epoch\nbatch_size = args.batch_size\nlearning_rate = args.learning_rate\nactivation = args.activation\noptimizer = args.optimizer\nloss = args.loss\ngpu = args.gpu\n\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = str(args.auto_mixed_precision)\n\n# the data, split between train and test sets\nx_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\nx_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n\ny_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\ny_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nK.set_image_data_format('channels_first')\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n    \nprint(input_shape, 'input_shape')\n    \nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, n_classes)\ny_test = keras.utils.to_categorical(y_test, n_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(neurons_1, kernel_size=(kernel_size_1, kernel_size_1),\n                activation=activation,\n                input_shape=input_shape))\nmodel.add(Conv2D(neurons_2, kernel_size=(kernel_size_2, kernel_size_2), \n                activation=activation))\nmodel.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\nmodel.add(Dropout(dropout_1))\nmodel.add(Flatten())\nmodel.add(Dense(neurons_3, \n                activation=activation))\nmodel.add(Dropout(dropout_2))\nmodel.add(Dense(n_classes, activation='softmax'))\n\nmodel.compile(loss=loss,\n            optimizer=optimizer_types[optimizer](learning_rate),\n            metrics=['accuracy'])\n\n# start an Azure ML run\nrun = Run.get_context()\n\nclass LogRunMetrics(Callback):\n    # callback at the end of every epoch\n    def on_epoch_end(self, epoch, log):\n        # log a value repeated which creates a list\n        run.log('Loss', log['loss'])\n        run.log('Accuracy', log['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test),\n          callbacks=[LogRunMetrics()])\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n\nrun.log(\"Final test loss\", score[0])\nrun.log('Final test accuracy', score[1])\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n# log an image\nplt.figure(figsize=(6, 3))\nplt.title('MNIST with Keras MLP ({} epochs)'.format(epochs), fontsize=14)\nplt.plot(history.history['accuracy'], 'b-', label='Accuracy', lw=4, alpha=0.5)\nplt.plot(history.history['loss'], 'r--', label='Loss', lw=4, alpha=0.5)\nplt.legend(fontsize=12)\nplt.grid(True)\n\nrun.log_image('Accuracy vs Loss', plot=plt)\n\n# save model\nos.makedirs('./outputs/model', exist_ok=True)\nmodel.save('./outputs/model/mnist.h5', include_optimizer=False)\n\n# serialize NN architecture to JSON\nmodel_json = model.to_json()\n# save model JSON\nwith open('./outputs/model/mnist.json', 'w') as f:\n    f.write(model_json)\n# save model weights\nmodel.save_weights('./outputs/model/mnist_weights.h5')\n\nprint(\"model saved in ./outputs/model folder\")\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Simple run with Create TensorFlow estimator & add Keras\nNext, we construct an `azureml.train.dnn.TensorFlow` estimator object, use the `gpucluster` as compute target, and pass the mount-point of the datastore to the training code as a parameter.\nThe TensorFlow estimator is providing a simple way of launching a TensorFlow training job on a compute target. It will automatically provide a docker image that has TensorFlow installed. In this case, we add `keras` package (for the Keras framework obviously), and `matplotlib` package for plotting a \"Loss vs. Accuracy\" chart and record it in run history."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import TensorFlow\n\nscript_params = {\n    '--data-folder': ds.path('mnist').as_mount(),\n    '--batch-size': 128,\n    '--epoch': 20,\n    '--neurons-1': 32,\n    '--neurons-2': 64,\n    '--neurons-3': 128,\n    '--kernel-size-1': 3,\n    '--kernel-size-2': 3,\n    '--pool-size': 2,\n    '--learning-rate': 0.001,\n    '--activation': 'relu',\n    '--optimizer': 'RMSprop',\n    '--loss': 'categorical_crossentropy',\n    '--dropout-1': 0.2,\n    '--dropout-2': 0.5,\n    '--gpu': gpu_count,\n    '--auto_mixed_precision': tensorcore_status\n}\n\nest = TensorFlow(source_directory=script_folder,\n                 script_params=script_params,\n                 compute_target=compute_target, \n                 pip_packages=['keras', 'matplotlib'],\n                 entry_script='mnist_cnn.py', \n                 use_gpu=True,\n                 max_run_duration_seconds=600)",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "WARNING - framework_version is not specified, defaulting to version 1.13.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And if you are curious, this is what the mounting point looks like:"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(ds.path('mnist').as_mount())",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "$AZUREML_DATAREFERENCE_3a8904cc65d343098e60878e7b7fac0c\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Submit job to run\nSubmit the estimator to the Azure ML experiment to kick off the execution."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(est)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Monitor the Run\nAs the Run is executed, it will go through the following stages:\n1. Preparing: A docker image is created matching the Python environment specified by the TensorFlow estimator and it will be uploaded to the workspace's Azure Container Registry. This step will only happen once for each Python environment -- the container will then be cached for subsequent runs. Creating and uploading the image takes about **5 minutes**. While the job is preparing, logs are streamed to the run history and can be viewed to monitor the progress of the image creation.\n\n2. Scaling: If the compute needs to be scaled up (i.e. the AmlCompute cluster requires more nodes to execute the run than currently available), the cluster will attempt to scale up in order to make the required amount of nodes available. Scaling typically takes about **5 minutes**.\n\n3. Running: All scripts in the script folder are uploaded to the compute target, data stores are mounted/copied and the `entry_script` is executed. While the job is running, stdout and the `./logs` folder are streamed to the run history and can be viewed to monitor the progress of the run.\n\n4. Post-Processing: The `./outputs` folder of the run is copied over to the run history\n\nThere are multiple ways to check the progress of a running job. We can use a Jupyter notebook widget. \n\n**Note: The widget will automatically update ever 10-15 seconds, always showing you the most up-to-date information about the run**"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4126187f47ce45cfa769fccdc922ea13",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also periodically check the status of the run object, and navigate to Azure portal to monitor the run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%time\nrun.wait_for_completion(show_output=True)",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "RunId: keras-mnist_1570403228_cf52f5a7\nWeb View: https://mlworkspace.azure.ai/portal/subscriptions/ca28e367-312d-4b07-a3fd-650390469a4e/resourceGroups/dahatakemlus/providers/Microsoft.MachineLearningServices/workspaces/dahatakemlus/experiments/keras-mnist/runs/keras-mnist_1570403228_cf52f5a7\n\nStreaming azureml-logs/55_azureml-execution-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt\n========================================================================================================================\n\n2019-10-06T23:09:48Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/dahatakemlus/azureml/keras-mnist_1570403228_cf52f5a7/mounts/workspaceblobstore\n2019-10-06T23:09:49Z Waiting for blobfs to be mounted at /mnt/batch/tasks/shared/LS_root/jobs/dahatakemlus/azureml/keras-mnist_1570403228_cf52f5a7/mounts/workspaceblobstore\n2019-10-06T23:09:49Z Successfully mounted azureml-blobstore-921bee64-c227-4e9a-9930-a9076e27ac6b container from dahatakestorage3a55ee0c9 account at /mnt/batch/tasks/shared/LS_root/jobs/dahatakemlus/azureml/keras-mnist_1570403228_cf52f5a7/mounts/workspaceblobstore\n2019-10-06T23:09:49Z No unmanaged file systems configured\n2019-10-06T23:09:49Z Starting output-watcher...\nLogin Succeeded\nUsing default tag: latest\nlatest: Pulling from azureml/azureml_d27c0495535dfab8cde5924dd9292416\nf7277927d38a: Pulling fs layer\n8d3eac894db4: Pulling fs layer\nedf72af6d627: Pulling fs layer\n3e4f86211d23: Pulling fs layer\nd6e9603ff777: Pulling fs layer\n5cad422780e2: Pulling fs layer\n8130687c8acb: Pulling fs layer\nc11e9246d621: Pulling fs layer\n0dfae24cbbd9: Pulling fs layer\n0bb049a6d391: Pulling fs layer\n988ac06e9a72: Pulling fs layer\n1d903ffe89fe: Pulling fs layer\nd6e9603ff777: Waiting\n4c5e9e6c7e14: Pulling fs layer\n7ebd3e323491: Pulling fs layer\n5cad422780e2: Waiting\n8796a8f9e018: Pulling fs layer\n8130687c8acb: Waiting\n362d975917fe: Pulling fs layer\n3e4f86211d23: Waiting\nc11e9246d621: Waiting\n0bb049a6d391: Waiting\n0dfae24cbbd9: Waiting\n4c5e9e6c7e14: Waiting\n988ac06e9a72: Waiting\n7ebd3e323491: Waiting\n8796a8f9e018: Waiting\n1d903ffe89fe: Waiting\na0018081f252: Pulling fs layer\n5aa4c659f893: Pulling fs layer\n362d975917fe: Waiting\n58edb0b5270b: Pulling fs layer\n47f02de77a6a: Pulling fs layer\n3ec797d8506f: Pulling fs layer\na0018081f252: Waiting\n9cc962388dab: Pulling fs layer\n5aa4c659f893: Waiting\n9236225877a4: Pulling fs layer\n47f02de77a6a: Waiting\n3ec797d8506f: Waiting\n9236225877a4: Waiting\nedf72af6d627: Download complete\n8d3eac894db4: Download complete\n3e4f86211d23: Verifying Checksum\n3e4f86211d23: Download complete\nf7277927d38a: Verifying Checksum\nf7277927d38a: Download complete\n8130687c8acb: Verifying Checksum\n8130687c8acb: Download complete\n5cad422780e2: Verifying Checksum\n5cad422780e2: Download complete\nd6e9603ff777: Verifying Checksum\nd6e9603ff777: Download complete\nf7277927d38a: Pull complete\n8d3eac894db4: Pull complete\nedf72af6d627: Pull complete\n3e4f86211d23: Pull complete\nd6e9603ff777: Pull complete\n5cad422780e2: Pull complete\n8130687c8acb: Pull complete\n0bb049a6d391: Verifying Checksum\n0bb049a6d391: Download complete\n988ac06e9a72: Verifying Checksum\n988ac06e9a72: Download complete\nc11e9246d621: Verifying Checksum\nc11e9246d621: Download complete\n1d903ffe89fe: Verifying Checksum\n1d903ffe89fe: Download complete\n0dfae24cbbd9: Verifying Checksum\n0dfae24cbbd9: Download complete\n4c5e9e6c7e14: Verifying Checksum\n4c5e9e6c7e14: Download complete\n362d975917fe: Verifying Checksum\n362d975917fe: Download complete\n7ebd3e323491: Verifying Checksum\n7ebd3e323491: Download complete\na0018081f252: Download complete\n58edb0b5270b: Verifying Checksum\n58edb0b5270b: Download complete\n5aa4c659f893: Download complete\n3ec797d8506f: Download complete\n47f02de77a6a: Verifying Checksum\n47f02de77a6a: Download complete\n8796a8f9e018: Verifying Checksum\n8796a8f9e018: Download complete\n9236225877a4: Verifying Checksum\n9236225877a4: Download complete\n9cc962388dab: Verifying Checksum\n9cc962388dab: Download complete\nc11e9246d621: Pull complete\n0dfae24cbbd9: Pull complete\n0bb049a6d391: Pull complete\n988ac06e9a72: Pull complete\n1d903ffe89fe: Pull complete\n4c5e9e6c7e14: Pull complete\n7ebd3e323491: Pull complete\n8796a8f9e018: Pull complete\n362d975917fe: Pull complete\na0018081f252: Pull complete\n5aa4c659f893: Pull complete\n58edb0b5270b: Pull complete\n47f02de77a6a: Pull complete\n3ec797d8506f: Pull complete\n9cc962388dab: Pull complete\n9236225877a4: Pull complete\nDigest: sha256:f4d03d54e731113ae961ad9e81999d99d73db08dbe794c619d7e9fd59c5205f5\nStatus: Downloaded newer image for dahatakemlusb0cad1b5.azurecr.io/azureml/azureml_d27c0495535dfab8cde5924dd9292416:latest\n3362c3063e9cf5f89e2f904f5891f650f6c25cfcdad029024a8b681824edb6c9\n\nStreaming azureml-logs/65_job_prep-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt\n===============================================================================================================\n\nbash: /azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/libtinfo.so.5: no version information available (required by bash)\nStarting job preparation. Current time:2019-10-06T23:11:45.089079\nExtracting the control code.\nCreating directory: azureml-logs/\nRetrieving project from URI: https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml-blobstore-921bee64-c227-4e9a-9930-a9076e27ac6b/azureml/project_zip_9dec96a9cfdc4ce5997ab23484ef3731?sv=2018-11-09&sr=b&sig=Xs6OPEhiaM7z7Nn1rDOHQ6VY7CrKFyyc2tv9e8kFYwk%3D&st=2019-10-06T22%3A57%3A37Z&se=2019-10-13T23%3A07%3A37Z&sp=r\nDownload from datastores if requested.\nDownload or mount from datasets if requested.\n\nStreaming azureml-logs/70_driver_log.txt\n========================================\n\nbash: /azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/libtinfo.so.5: no version information available (required by bash)\nbash: /azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/libtinfo.so.5: no version information available (required by bash)\nStarting the daemon thread to refresh tokens in background for process with pid = 139\nEntering Run History Context Manager.\nUsing TensorFlow backend.\n/azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n(1, 28, 28) input_shape\nx_train shape: (60000, 1, 28, 28)\n60000 train samples\n10000 test samples\nWARNING:tensorflow:From /azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n2019-10-06 23:11:57.330431: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2019-10-06 23:11:57.765858: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x61e8ff0 executing computations on platform CUDA. Devices:\n2019-10-06 23:11:57.765958: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\n2019-10-06 23:11:57.775672: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2593985000 Hz\n2019-10-06 23:11:57.776795: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x62513d0 executing computations on platform Host. Devices:\n2019-10-06 23:11:57.776864: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n2019-10-06 23:11:57.777032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\npciBusID: e58a:00:00.0\ntotalMemory: 15.75GiB freeMemory: 15.44GiB\n2019-10-06 23:11:57.777096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n2019-10-06 23:11:57.778783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n2019-10-06 23:11:57.778843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n2019-10-06 23:11:57.778886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n2019-10-06 23:11:57.778990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15022 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: e58a:00:00.0, compute capability: 7.0)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "WARNING:tensorflow:From /azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/20\n2019-10-06 23:11:59.324243: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n\n  128/60000 [..............................] - ETA: 14:43 - loss: 2.3025 - accuracy: 0.1250\n 1536/60000 [..............................] - ETA: 1:13 - loss: 2.3025 - accuracy: 0.0970 \n 3072/60000 [>.............................] - ETA: 36s - loss: 2.3019 - accuracy: 0.1029 \n 4608/60000 [=>............................] - ETA: 24s - loss: 2.2999 - accuracy: 0.1185\n 6144/60000 [==>...........................] - ETA: 18s - loss: 2.2918 - accuracy: 0.1460\n 7680/60000 [==>...........................] - ETA: 14s - loss: 2.2750 - accuracy: 0.1763\n 9216/60000 [===>..........................] - ETA: 12s - loss: 2.2480 - accuracy: 0.2096\n10752/60000 [====>.........................] - ETA: 10s - loss: 2.2049 - accuracy: 0.2426\n12288/60000 [=====>........................] - ETA: 8s - loss: 2.1520 - accuracy: 0.2769 \n13824/60000 [=====>........................] - ETA: 7s - loss: 2.0908 - accuracy: 0.3063\n15360/60000 [======>.......................] - ETA: 7s - loss: 2.0282 - accuracy: 0.3329\n16896/60000 [=======>......................] - ETA: 6s - loss: 1.9618 - accuracy: 0.3585\n18432/60000 [========>.....................] - ETA: 5s - loss: 1.8927 - accuracy: 0.3837\n19968/60000 [========>.....................] - ETA: 5s - loss: 1.8296 - accuracy: 0.4040\n21504/60000 [=========>....................] - ETA: 4s - loss: 1.7649 - accuracy: 0.4253\n23040/60000 [==========>...................] - ETA: 4s - loss: 1.7071 - accuracy: 0.4441\n24576/60000 [===========>..................] - ETA: 3s - loss: 1.6527 - accuracy: 0.4623\n26112/60000 [============>.................] - ETA: 3s - loss: 1.6030 - accuracy: 0.4783\n27648/60000 [============>.................] - ETA: 3s - loss: 1.5565 - accuracy: 0.4935\n29184/60000 [=============>................] - ETA: 3s - loss: 1.5124 - accuracy: 0.5075\n30720/60000 [==============>...............] - ETA: 2s - loss: 1.4732 - accuracy: 0.5211\n32256/60000 [===============>..............] - ETA: 2s - loss: 1.4343 - accuracy: 0.5336\n33792/60000 [===============>..............] - ETA: 2s - loss: 1.3976 - accuracy: 0.5462\n35328/60000 [================>.............] - ETA: 2s - loss: 1.3645 - accuracy: 0.5574\n36864/60000 [=================>............] - ETA: 1s - loss: 1.3335 - accuracy: 0.5675\n38400/60000 [==================>...........] - ETA: 1s - loss: 1.3056 - accuracy: 0.5765\n39936/60000 [==================>...........] - ETA: 1s - loss: 1.2772 - accuracy: 0.5862\n41344/60000 [===================>..........] - ETA: 1s - loss: 1.2532 - accuracy: 0.5941\n42880/60000 [====================>.........] - ETA: 1s - loss: 1.2282 - accuracy: 0.6025\n44416/60000 [=====================>........] - ETA: 1s - loss: 1.2046 - accuracy: 0.6104\n45952/60000 [=====================>........] - ETA: 1s - loss: 1.1833 - accuracy: 0.6172\n47488/60000 [======================>.......] - ETA: 0s - loss: 1.1631 - accuracy: 0.6239\n49024/60000 [=======================>......] - ETA: 0s - loss: 1.1430 - accuracy: 0.6307\n50560/60000 [========================>.....] - ETA: 0s - loss: 1.1241 - accuracy: 0.6372\n52096/60000 [=========================>....] - ETA: 0s - loss: 1.1064 - accuracy: 0.6434\n53632/60000 [=========================>....] - ETA: 0s - loss: 1.0898 - accuracy: 0.6490\n55168/60000 [==========================>...] - ETA: 0s - loss: 1.0718 - accuracy: 0.6548\n56704/60000 [===========================>..] - ETA: 0s - loss: 1.0562 - accuracy: 0.6599\n58240/60000 [============================>.] - ETA: 0s - loss: 1.0407 - accuracy: 0.6653\n59776/60000 [============================>.] - ETA: 0s - loss: 1.0260 - accuracy: 0.6700\n60000/60000 [==============================] - 4s 69us/step - loss: 1.0239 - accuracy: 0.6707 - val_loss: 0.3324 - val_accuracy: 0.9030\nEpoch 2/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.4584 - accuracy: 0.8594\n 1664/60000 [..............................] - ETA: 2s - loss: 0.4599 - accuracy: 0.8498\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.4592 - accuracy: 0.8512\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.4587 - accuracy: 0.8522\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.4631 - accuracy: 0.8549\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.4587 - accuracy: 0.8573\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.4566 - accuracy: 0.8594\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.4577 - accuracy: 0.8595\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.4539 - accuracy: 0.8598\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.4513 - accuracy: 0.8594\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.4473 - accuracy: 0.8603\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.4471 - accuracy: 0.8612\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.4460 - accuracy: 0.8622\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.4451 - accuracy: 0.8627\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.4440 - accuracy: 0.8631\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.4429 - accuracy: 0.8647\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.4418 - accuracy: 0.8653\n26240/60000 [============>.................] - ETA: 1s - loss: 0.4396 - accuracy: 0.8660\n27776/60000 [============>.................] - ETA: 1s - loss: 0.4371 - accuracy: 0.8670\n29312/60000 [=============>................] - ETA: 1s - loss: 0.4351 - accuracy: 0.8673\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.4325 - accuracy: 0.8683\n32384/60000 [===============>..............] - ETA: 0s - loss: 0.4294 - accuracy: 0.8693\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.4272 - accuracy: 0.8701\n35328/60000 [================>.............] - ETA: 0s - loss: 0.4260 - accuracy: 0.8704\n36864/60000 [=================>............] - ETA: 0s - loss: 0.4264 - accuracy: 0.8703\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.4250 - accuracy: 0.8711\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.4245 - accuracy: 0.8712\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.4244 - accuracy: 0.8711\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.4233 - accuracy: 0.8714\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.4227 - accuracy: 0.8719\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.4219 - accuracy: 0.8719\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.4199 - accuracy: 0.8722\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.4185 - accuracy: 0.8726\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.4178 - accuracy: 0.8730\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.4175 - accuracy: 0.8732\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.4162 - accuracy: 0.8735\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.4142 - accuracy: 0.8739\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.4152 - accuracy: 0.8738\n58368/60000 [============================>.] - ETA: 0s - loss: 0.4147 - accuracy: 0.8736\n59904/60000 [============================>.] - ETA: 0s - loss: 0.4141 - accuracy: 0.8737\n60000/60000 [==============================] - 2s 36us/step - loss: 0.4138 - accuracy: 0.8738 - val_loss: 0.2282 - val_accuracy: 0.9290\nEpoch 3/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.2977 - accuracy: 0.9297\n 1664/60000 [..............................] - ETA: 2s - loss: 0.3803 - accuracy: 0.8900\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.3763 - accuracy: 0.8906\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.3718 - accuracy: 0.8866\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.3795 - accuracy: 0.8870\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.3753 - accuracy: 0.8878\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.3707 - accuracy: 0.8893\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.3666 - accuracy: 0.8893\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.3693 - accuracy: 0.8885\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.3670 - accuracy: 0.8888\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.3693 - accuracy: 0.8881\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.3718 - accuracy: 0.8876\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.3717 - accuracy: 0.8884\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.3688 - accuracy: 0.8889\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.3662 - accuracy: 0.8893\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.3632 - accuracy: 0.8894\n24576/60000 [===========>..................] - ETA: 1s - loss: 0.3614 - accuracy: 0.8900\n26112/60000 [============>.................] - ETA: 1s - loss: 0.3618 - accuracy: 0.8897\n27648/60000 [============>.................] - ETA: 1s - loss: 0.3598 - accuracy: 0.8902\n29184/60000 [=============>................] - ETA: 1s - loss: 0.3581 - accuracy: 0.8906\n30720/60000 [==============>...............] - ETA: 0s - loss: 0.3569 - accuracy: 0.8913\n32256/60000 [===============>..............] - ETA: 0s - loss: 0.3566 - accuracy: 0.8913\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.3556 - accuracy: 0.8917\n35328/60000 [================>.............] - ETA: 0s - loss: 0.3537 - accuracy: 0.8925\n36864/60000 [=================>............] - ETA: 0s - loss: 0.3536 - accuracy: 0.8922\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.3528 - accuracy: 0.8927\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.3506 - accuracy: 0.8934\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.3507 - accuracy: 0.8937\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8938\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.3483 - accuracy: 0.8940\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.3490 - accuracy: 0.8939\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.3481 - accuracy: 0.8942\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.3477 - accuracy: 0.8944\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.3474 - accuracy: 0.8947\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.3474 - accuracy: 0.8948\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.3471 - accuracy: 0.8948\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.3483 - accuracy: 0.8945\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.3481 - accuracy: 0.8946\n58368/60000 [============================>.] - ETA: 0s - loss: 0.3473 - accuracy: 0.8948\n59904/60000 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.8953\n60000/60000 [==============================] - 2s 36us/step - loss: 0.3455 - accuracy: 0.8954 - val_loss: 0.1938 - val_accuracy: 0.9420\nEpoch 4/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.3593 - accuracy: 0.8906\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": " 1664/60000 [..............................] - ETA: 1s - loss: 0.3149 - accuracy: 0.8948\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.3242 - accuracy: 0.9000\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.3102 - accuracy: 0.9054\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.3150 - accuracy: 0.9050\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.3094 - accuracy: 0.9059\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.3116 - accuracy: 0.9044\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.3085 - accuracy: 0.9060\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.3107 - accuracy: 0.9061\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.3119 - accuracy: 0.9055\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.3121 - accuracy: 0.9057\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.3146 - accuracy: 0.9060\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.3146 - accuracy: 0.9060\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.3163 - accuracy: 0.9057\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.3177 - accuracy: 0.9046\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.3157 - accuracy: 0.9054\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.3127 - accuracy: 0.9065\n26240/60000 [============>.................] - ETA: 1s - loss: 0.3121 - accuracy: 0.9065\n27776/60000 [============>.................] - ETA: 1s - loss: 0.3131 - accuracy: 0.9061\n29184/60000 [=============>................] - ETA: 1s - loss: 0.3112 - accuracy: 0.9064\n30720/60000 [==============>...............] - ETA: 0s - loss: 0.3104 - accuracy: 0.9069\n32256/60000 [===============>..............] - ETA: 0s - loss: 0.3084 - accuracy: 0.9071\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.3076 - accuracy: 0.9072\n35328/60000 [================>.............] - ETA: 0s - loss: 0.3091 - accuracy: 0.9067\n36864/60000 [=================>............] - ETA: 0s - loss: 0.3086 - accuracy: 0.9068\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.3093 - accuracy: 0.9067\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.3101 - accuracy: 0.9064\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.3104 - accuracy: 0.9063\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.3116 - accuracy: 0.9061\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.3105 - accuracy: 0.9065\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.3115 - accuracy: 0.9063\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.3108 - accuracy: 0.9066\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.3087 - accuracy: 0.9071\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.3083 - accuracy: 0.9074\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.3084 - accuracy: 0.9073\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.3081 - accuracy: 0.9073\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.3081 - accuracy: 0.9075\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.3082 - accuracy: 0.9074\n58368/60000 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.9070\n59904/60000 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.9073\n60000/60000 [==============================] - 2s 36us/step - loss: 0.3089 - accuracy: 0.9074 - val_loss: 0.1723 - val_accuracy: 0.9441\nEpoch 5/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1850 - accuracy: 0.9531\n 1664/60000 [..............................] - ETA: 1s - loss: 0.3240 - accuracy: 0.9153\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.3121 - accuracy: 0.9122\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.3042 - accuracy: 0.9134\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.2994 - accuracy: 0.9112\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.2981 - accuracy: 0.9119\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.2953 - accuracy: 0.9119\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.2935 - accuracy: 0.9115\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.2903 - accuracy: 0.9131\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.2890 - accuracy: 0.9143\n15232/60000 [======>.......................] - ETA: 1s - loss: 0.2884 - accuracy: 0.9149\n16768/60000 [=======>......................] - ETA: 1s - loss: 0.2920 - accuracy: 0.9136\n18304/60000 [========>.....................] - ETA: 1s - loss: 0.2935 - accuracy: 0.9134\n19840/60000 [========>.....................] - ETA: 1s - loss: 0.2908 - accuracy: 0.9145\n21376/60000 [=========>....................] - ETA: 1s - loss: 0.2902 - accuracy: 0.9146\n22912/60000 [==========>...................] - ETA: 1s - loss: 0.2897 - accuracy: 0.9150\n24448/60000 [===========>..................] - ETA: 1s - loss: 0.2889 - accuracy: 0.9152\n25984/60000 [===========>..................] - ETA: 1s - loss: 0.2878 - accuracy: 0.9151\n27520/60000 [============>.................] - ETA: 1s - loss: 0.2862 - accuracy: 0.9156\n29056/60000 [=============>................] - ETA: 1s - loss: 0.2850 - accuracy: 0.9154\n30592/60000 [==============>...............] - ETA: 0s - loss: 0.2854 - accuracy: 0.9151\n32128/60000 [===============>..............] - ETA: 0s - loss: 0.2857 - accuracy: 0.9149\n33664/60000 [===============>..............] - ETA: 0s - loss: 0.2856 - accuracy: 0.9150\n35200/60000 [================>.............] - ETA: 0s - loss: 0.2849 - accuracy: 0.9154\n36736/60000 [=================>............] - ETA: 0s - loss: 0.2845 - accuracy: 0.9156\n38272/60000 [==================>...........] - ETA: 0s - loss: 0.2833 - accuracy: 0.9162\n39680/60000 [==================>...........] - ETA: 0s - loss: 0.2829 - accuracy: 0.9162\n41216/60000 [===================>..........] - ETA: 0s - loss: 0.2817 - accuracy: 0.9166\n42752/60000 [====================>.........] - ETA: 0s - loss: 0.2823 - accuracy: 0.9167\n44288/60000 [=====================>........] - ETA: 0s - loss: 0.2827 - accuracy: 0.9168\n45824/60000 [=====================>........] - ETA: 0s - loss: 0.2839 - accuracy: 0.9168\n47360/60000 [======================>.......] - ETA: 0s - loss: 0.2838 - accuracy: 0.9169\n48896/60000 [=======================>......] - ETA: 0s - loss: 0.2827 - accuracy: 0.9170\n50432/60000 [========================>.....] - ETA: 0s - loss: 0.2829 - accuracy: 0.9170\n51968/60000 [========================>.....] - ETA: 0s - loss: 0.2833 - accuracy: 0.9171\n53504/60000 [=========================>....] - ETA: 0s - loss: 0.2841 - accuracy: 0.9168\n55040/60000 [==========================>...] - ETA: 0s - loss: 0.2840 - accuracy: 0.9170\n56576/60000 [===========================>..] - ETA: 0s - loss: 0.2835 - accuracy: 0.9170\n58112/60000 [============================>.] - ETA: 0s - loss: 0.2843 - accuracy: 0.9166\n59648/60000 [============================>.] - ETA: 0s - loss: 0.2834 - accuracy: 0.9168\n60000/60000 [==============================] - 2s 36us/step - loss: 0.2832 - accuracy: 0.9168 - val_loss: 0.1537 - val_accuracy: 0.9526\nEpoch 6/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.2626 - accuracy: 0.9141\n 1536/60000 [..............................] - ETA: 2s - loss: 0.3004 - accuracy: 0.9141\n 3072/60000 [>.............................] - ETA: 2s - loss: 0.2727 - accuracy: 0.9212\n 4608/60000 [=>............................] - ETA: 1s - loss: 0.2822 - accuracy: 0.9178\n 6144/60000 [==>...........................] - ETA: 1s - loss: 0.2797 - accuracy: 0.9175\n 7680/60000 [==>...........................] - ETA: 1s - loss: 0.2771 - accuracy: 0.9172\n 9216/60000 [===>..........................] - ETA: 1s - loss: 0.2786 - accuracy: 0.9176\n10752/60000 [====>.........................] - ETA: 1s - loss: 0.2721 - accuracy: 0.9185\n12288/60000 [=====>........................] - ETA: 1s - loss: 0.2729 - accuracy: 0.9184\n13824/60000 [=====>........................] - ETA: 1s - loss: 0.2765 - accuracy: 0.9165\n15360/60000 [======>.......................] - ETA: 1s - loss: 0.2753 - accuracy: 0.9168\n16896/60000 [=======>......................] - ETA: 1s - loss: 0.2730 - accuracy: 0.9173\n18432/60000 [========>.....................] - ETA: 1s - loss: 0.2715 - accuracy: 0.9179\n19968/60000 [========>.....................] - ETA: 1s - loss: 0.2698 - accuracy: 0.9178\n21504/60000 [=========>....................] - ETA: 1s - loss: 0.2689 - accuracy: 0.9184\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.2685 - accuracy: 0.9180\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.2685 - accuracy: 0.9181\n26240/60000 [============>.................] - ETA: 1s - loss: 0.2683 - accuracy: 0.9186\n27776/60000 [============>.................] - ETA: 1s - loss: 0.2701 - accuracy: 0.9181\n29312/60000 [=============>................] - ETA: 1s - loss: 0.2695 - accuracy: 0.9184\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.2681 - accuracy: 0.9186\n32384/60000 [===============>..............] - ETA: 0s - loss: 0.2659 - accuracy: 0.9190\n33920/60000 [===============>..............] - ETA: 0s - loss: 0.2653 - accuracy: 0.9192\n35456/60000 [================>.............] - ETA: 0s - loss: 0.2646 - accuracy: 0.9196\n36864/60000 [=================>............] - ETA: 0s - loss: 0.2654 - accuracy: 0.9195\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.2656 - accuracy: 0.9195\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.2651 - accuracy: 0.9197\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.2656 - accuracy: 0.9197\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.2659 - accuracy: 0.9201\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.2653 - accuracy: 0.9205\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.2643 - accuracy: 0.9210\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.2630 - accuracy: 0.9212\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.2638 - accuracy: 0.9210\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.2625 - accuracy: 0.9213\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "52224/60000 [=========================>....] - ETA: 0s - loss: 0.2621 - accuracy: 0.9212\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.2614 - accuracy: 0.9216\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.2608 - accuracy: 0.9218\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.2599 - accuracy: 0.9223\n58368/60000 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.9224\n59904/60000 [============================>.] - ETA: 0s - loss: 0.2600 - accuracy: 0.9225\n60000/60000 [==============================] - 2s 36us/step - loss: 0.2597 - accuracy: 0.9226 - val_loss: 0.1395 - val_accuracy: 0.9571\nEpoch 7/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1777 - accuracy: 0.9453\n 1664/60000 [..............................] - ETA: 1s - loss: 0.2381 - accuracy: 0.9255\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.2572 - accuracy: 0.9247\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.2533 - accuracy: 0.9257\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.2499 - accuracy: 0.9257\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.2521 - accuracy: 0.9253\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.2551 - accuracy: 0.9236\n11008/60000 [====>.........................] - ETA: 1s - loss: 0.2495 - accuracy: 0.9240\n12544/60000 [=====>........................] - ETA: 1s - loss: 0.2488 - accuracy: 0.9252\n14080/60000 [======>.......................] - ETA: 1s - loss: 0.2468 - accuracy: 0.9261\n15616/60000 [======>.......................] - ETA: 1s - loss: 0.2440 - accuracy: 0.9273\n17152/60000 [=======>......................] - ETA: 1s - loss: 0.2469 - accuracy: 0.9272\n18688/60000 [========>.....................] - ETA: 1s - loss: 0.2470 - accuracy: 0.9272\n20224/60000 [=========>....................] - ETA: 1s - loss: 0.2472 - accuracy: 0.9276\n21760/60000 [=========>....................] - ETA: 1s - loss: 0.2444 - accuracy: 0.9284\n23296/60000 [==========>...................] - ETA: 1s - loss: 0.2430 - accuracy: 0.9283\n24832/60000 [===========>..................] - ETA: 1s - loss: 0.2447 - accuracy: 0.9284\n26368/60000 [============>.................] - ETA: 1s - loss: 0.2435 - accuracy: 0.9285\n27904/60000 [============>.................] - ETA: 1s - loss: 0.2431 - accuracy: 0.9288\n29440/60000 [=============>................] - ETA: 1s - loss: 0.2439 - accuracy: 0.9283\n30976/60000 [==============>...............] - ETA: 0s - loss: 0.2437 - accuracy: 0.9285\n32512/60000 [===============>..............] - ETA: 0s - loss: 0.2463 - accuracy: 0.9279\n34048/60000 [================>.............] - ETA: 0s - loss: 0.2449 - accuracy: 0.9282\n35584/60000 [================>.............] - ETA: 0s - loss: 0.2435 - accuracy: 0.9286\n37120/60000 [=================>............] - ETA: 0s - loss: 0.2423 - accuracy: 0.9290\n38656/60000 [==================>...........] - ETA: 0s - loss: 0.2416 - accuracy: 0.9292\n40192/60000 [===================>..........] - ETA: 0s - loss: 0.2406 - accuracy: 0.9292\n41728/60000 [===================>..........] - ETA: 0s - loss: 0.2400 - accuracy: 0.9295\n43264/60000 [====================>.........] - ETA: 0s - loss: 0.2392 - accuracy: 0.9295\n44800/60000 [=====================>........] - ETA: 0s - loss: 0.2380 - accuracy: 0.9299\n46336/60000 [======================>.......] - ETA: 0s - loss: 0.2393 - accuracy: 0.9294\n47872/60000 [======================>.......] - ETA: 0s - loss: 0.2393 - accuracy: 0.9294\n49408/60000 [=======================>......] - ETA: 0s - loss: 0.2388 - accuracy: 0.9295\n50944/60000 [========================>.....] - ETA: 0s - loss: 0.2393 - accuracy: 0.9296\n52480/60000 [=========================>....] - ETA: 0s - loss: 0.2389 - accuracy: 0.9297\n54016/60000 [==========================>...] - ETA: 0s - loss: 0.2384 - accuracy: 0.9299\n55552/60000 [==========================>...] - ETA: 0s - loss: 0.2377 - accuracy: 0.9302\n57088/60000 [===========================>..] - ETA: 0s - loss: 0.2381 - accuracy: 0.9302\n58624/60000 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9301\n60000/60000 [==============================] - 2s 36us/step - loss: 0.2379 - accuracy: 0.9301 - val_loss: 0.1265 - val_accuracy: 0.9617\nEpoch 8/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1510 - accuracy: 0.9531\n 1664/60000 [..............................] - ETA: 1s - loss: 0.2113 - accuracy: 0.9315\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.2064 - accuracy: 0.9347\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.2052 - accuracy: 0.9367\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.2045 - accuracy: 0.9389\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.2043 - accuracy: 0.9392\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.2076 - accuracy: 0.9384\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.2136 - accuracy: 0.9381\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.2156 - accuracy: 0.9372\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.2159 - accuracy: 0.9358\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.2193 - accuracy: 0.9349\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.2193 - accuracy: 0.9349\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.2207 - accuracy: 0.9344\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.2203 - accuracy: 0.9346\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.2201 - accuracy: 0.9345\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.2222 - accuracy: 0.9340\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.2215 - accuracy: 0.9345\n26240/60000 [============>.................] - ETA: 1s - loss: 0.2212 - accuracy: 0.9347\n27776/60000 [============>.................] - ETA: 1s - loss: 0.2214 - accuracy: 0.9343\n29312/60000 [=============>................] - ETA: 1s - loss: 0.2227 - accuracy: 0.9342\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.2239 - accuracy: 0.9337\n32384/60000 [===============>..............] - ETA: 0s - loss: 0.2219 - accuracy: 0.9340\n33920/60000 [===============>..............] - ETA: 0s - loss: 0.2219 - accuracy: 0.9340\n35456/60000 [================>.............] - ETA: 0s - loss: 0.2222 - accuracy: 0.9336\n36864/60000 [=================>............] - ETA: 0s - loss: 0.2213 - accuracy: 0.9339\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.2195 - accuracy: 0.9343\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.2195 - accuracy: 0.9342\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.2197 - accuracy: 0.9339\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.2198 - accuracy: 0.9340\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.2196 - accuracy: 0.9342\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.2194 - accuracy: 0.9344\n47488/60000 [======================>.......] - ETA: 0s - loss: 0.2184 - accuracy: 0.9347\n49024/60000 [=======================>......] - ETA: 0s - loss: 0.2188 - accuracy: 0.9346\n50560/60000 [========================>.....] - ETA: 0s - loss: 0.2192 - accuracy: 0.9346\n52096/60000 [=========================>....] - ETA: 0s - loss: 0.2188 - accuracy: 0.9346\n53632/60000 [=========================>....] - ETA: 0s - loss: 0.2188 - accuracy: 0.9346\n55168/60000 [==========================>...] - ETA: 0s - loss: 0.2189 - accuracy: 0.9347\n56576/60000 [===========================>..] - ETA: 0s - loss: 0.2182 - accuracy: 0.9351\n58112/60000 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9349\n59648/60000 [============================>.] - ETA: 0s - loss: 0.2182 - accuracy: 0.9351\n60000/60000 [==============================] - 2s 37us/step - loss: 0.2178 - accuracy: 0.9352 - val_loss: 0.1185 - val_accuracy: 0.9651\nEpoch 9/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.3470 - accuracy: 0.8984\n 1408/60000 [..............................] - ETA: 2s - loss: 0.2152 - accuracy: 0.9411\n 2688/60000 [>.............................] - ETA: 2s - loss: 0.2185 - accuracy: 0.9360\n 4224/60000 [=>............................] - ETA: 2s - loss: 0.2214 - accuracy: 0.9351\n 5760/60000 [=>............................] - ETA: 2s - loss: 0.2197 - accuracy: 0.9345\n 7296/60000 [==>...........................] - ETA: 1s - loss: 0.2153 - accuracy: 0.9339\n 8832/60000 [===>..........................] - ETA: 1s - loss: 0.2112 - accuracy: 0.9343\n10368/60000 [====>.........................] - ETA: 1s - loss: 0.2100 - accuracy: 0.9359\n11904/60000 [====>.........................] - ETA: 1s - loss: 0.2073 - accuracy: 0.9371\n13440/60000 [=====>........................] - ETA: 1s - loss: 0.2086 - accuracy: 0.9372\n14976/60000 [======>.......................] - ETA: 1s - loss: 0.2066 - accuracy: 0.9377\n16512/60000 [=======>......................] - ETA: 1s - loss: 0.2042 - accuracy: 0.9379\n18048/60000 [========>.....................] - ETA: 1s - loss: 0.2032 - accuracy: 0.9384\n19584/60000 [========>.....................] - ETA: 1s - loss: 0.1989 - accuracy: 0.9397\n21120/60000 [=========>....................] - ETA: 1s - loss: 0.1993 - accuracy: 0.9395\n22656/60000 [==========>...................] - ETA: 1s - loss: 0.2001 - accuracy: 0.9396\n24192/60000 [===========>..................] - ETA: 1s - loss: 0.2014 - accuracy: 0.9390\n25728/60000 [===========>..................] - ETA: 1s - loss: 0.2012 - accuracy: 0.9396\n27264/60000 [============>.................] - ETA: 1s - loss: 0.2004 - accuracy: 0.9397\n28800/60000 [=============>................] - ETA: 1s - loss: 0.1995 - accuracy: 0.9398\n30336/60000 [==============>...............] - ETA: 1s - loss: 0.2007 - accuracy: 0.9398\n31872/60000 [==============>...............] - ETA: 0s - loss: 0.2000 - accuracy: 0.9401\n33408/60000 [===============>..............] - ETA: 0s - loss: 0.2010 - accuracy: 0.9399\n34944/60000 [================>.............] - ETA: 0s - loss: 0.1996 - accuracy: 0.9403\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "36480/60000 [=================>............] - ETA: 0s - loss: 0.2002 - accuracy: 0.9401\n38016/60000 [==================>...........] - ETA: 0s - loss: 0.1992 - accuracy: 0.9406\n39296/60000 [==================>...........] - ETA: 0s - loss: 0.1977 - accuracy: 0.9411\n40832/60000 [===================>..........] - ETA: 0s - loss: 0.1983 - accuracy: 0.9408\n42368/60000 [====================>.........] - ETA: 0s - loss: 0.1985 - accuracy: 0.9408\n43904/60000 [====================>.........] - ETA: 0s - loss: 0.1978 - accuracy: 0.9409\n45440/60000 [=====================>........] - ETA: 0s - loss: 0.1969 - accuracy: 0.9412\n46976/60000 [======================>.......] - ETA: 0s - loss: 0.1964 - accuracy: 0.9412\n48512/60000 [=======================>......] - ETA: 0s - loss: 0.1968 - accuracy: 0.9414\n50048/60000 [========================>.....] - ETA: 0s - loss: 0.1969 - accuracy: 0.9414\n51584/60000 [========================>.....] - ETA: 0s - loss: 0.1970 - accuracy: 0.9413\n53120/60000 [=========================>....] - ETA: 0s - loss: 0.1969 - accuracy: 0.9415\n54656/60000 [==========================>...] - ETA: 0s - loss: 0.1966 - accuracy: 0.9416\n56192/60000 [===========================>..] - ETA: 0s - loss: 0.1969 - accuracy: 0.9416\n57728/60000 [===========================>..] - ETA: 0s - loss: 0.1970 - accuracy: 0.9417\n59264/60000 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9416\n60000/60000 [==============================] - 2s 37us/step - loss: 0.1967 - accuracy: 0.9418 - val_loss: 0.0974 - val_accuracy: 0.9702\nEpoch 10/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1525 - accuracy: 0.9375\n 1664/60000 [..............................] - ETA: 2s - loss: 0.2054 - accuracy: 0.9315\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.2025 - accuracy: 0.9353\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1936 - accuracy: 0.9388\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1837 - accuracy: 0.9407\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1839 - accuracy: 0.9415\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1853 - accuracy: 0.9419\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1911 - accuracy: 0.9404\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1917 - accuracy: 0.9403\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1901 - accuracy: 0.9413\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1916 - accuracy: 0.9412\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1893 - accuracy: 0.9423\n18432/60000 [========>.....................] - ETA: 1s - loss: 0.1884 - accuracy: 0.9430\n19968/60000 [========>.....................] - ETA: 1s - loss: 0.1859 - accuracy: 0.9439\n21504/60000 [=========>....................] - ETA: 1s - loss: 0.1854 - accuracy: 0.9443\n23040/60000 [==========>...................] - ETA: 1s - loss: 0.1873 - accuracy: 0.9438\n24576/60000 [===========>..................] - ETA: 1s - loss: 0.1888 - accuracy: 0.9438\n26112/60000 [============>.................] - ETA: 1s - loss: 0.1870 - accuracy: 0.9442\n27648/60000 [============>.................] - ETA: 1s - loss: 0.1859 - accuracy: 0.9447\n29184/60000 [=============>................] - ETA: 1s - loss: 0.1843 - accuracy: 0.9453\n30720/60000 [==============>...............] - ETA: 0s - loss: 0.1831 - accuracy: 0.9457\n32256/60000 [===============>..............] - ETA: 0s - loss: 0.1840 - accuracy: 0.9456\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.1821 - accuracy: 0.9460\n35328/60000 [================>.............] - ETA: 0s - loss: 0.1817 - accuracy: 0.9459\n36864/60000 [=================>............] - ETA: 0s - loss: 0.1819 - accuracy: 0.9457\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.1826 - accuracy: 0.9457\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.1823 - accuracy: 0.9459\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.1826 - accuracy: 0.9458\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.1819 - accuracy: 0.9462\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.1818 - accuracy: 0.9462\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.1804 - accuracy: 0.9465\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.1807 - accuracy: 0.9465\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.1805 - accuracy: 0.9465\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.1803 - accuracy: 0.9466\n52096/60000 [=========================>....] - ETA: 0s - loss: 0.1810 - accuracy: 0.9465\n53632/60000 [=========================>....] - ETA: 0s - loss: 0.1817 - accuracy: 0.9465\n55168/60000 [==========================>...] - ETA: 0s - loss: 0.1803 - accuracy: 0.9469\n56704/60000 [===========================>..] - ETA: 0s - loss: 0.1811 - accuracy: 0.9469\n58240/60000 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9468\n59776/60000 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9469\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1806 - accuracy: 0.9469 - val_loss: 0.0932 - val_accuracy: 0.9717\nEpoch 11/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.2140 - accuracy: 0.9531\n 1664/60000 [..............................] - ETA: 2s - loss: 0.1782 - accuracy: 0.9501\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1798 - accuracy: 0.9491\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1776 - accuracy: 0.9485\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1826 - accuracy: 0.9471\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1814 - accuracy: 0.9471\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1808 - accuracy: 0.9476\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1776 - accuracy: 0.9483\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1804 - accuracy: 0.9482\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1801 - accuracy: 0.9484\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1791 - accuracy: 0.9481\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1786 - accuracy: 0.9473\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.1799 - accuracy: 0.9471\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.1796 - accuracy: 0.9473\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.1777 - accuracy: 0.9476\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.1748 - accuracy: 0.9480\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.1757 - accuracy: 0.9475\n26240/60000 [============>.................] - ETA: 1s - loss: 0.1757 - accuracy: 0.9477\n27776/60000 [============>.................] - ETA: 1s - loss: 0.1741 - accuracy: 0.9482\n29312/60000 [=============>................] - ETA: 1s - loss: 0.1740 - accuracy: 0.9482\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.1743 - accuracy: 0.9482\n32384/60000 [===============>..............] - ETA: 0s - loss: 0.1745 - accuracy: 0.9479\n33920/60000 [===============>..............] - ETA: 0s - loss: 0.1741 - accuracy: 0.9482\n35456/60000 [================>.............] - ETA: 0s - loss: 0.1730 - accuracy: 0.9486\n36992/60000 [=================>............] - ETA: 0s - loss: 0.1726 - accuracy: 0.9488\n38528/60000 [==================>...........] - ETA: 0s - loss: 0.1728 - accuracy: 0.9487\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.1713 - accuracy: 0.9491\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.1711 - accuracy: 0.9492\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.1704 - accuracy: 0.9496\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.1703 - accuracy: 0.9495\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.1700 - accuracy: 0.9496\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.1693 - accuracy: 0.9497\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.1696 - accuracy: 0.9498\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.1699 - accuracy: 0.9496\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.1690 - accuracy: 0.9496\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.1686 - accuracy: 0.9498\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.1690 - accuracy: 0.9497\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.1687 - accuracy: 0.9499\n58368/60000 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9504\n59776/60000 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.9505\n60000/60000 [==============================] - 2s 37us/step - loss: 0.1677 - accuracy: 0.9506 - val_loss: 0.0859 - val_accuracy: 0.9743\nEpoch 12/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1422 - accuracy: 0.9609\n 1664/60000 [..............................] - ETA: 1s - loss: 0.1652 - accuracy: 0.9501\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1716 - accuracy: 0.9491\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1742 - accuracy: 0.9476\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1706 - accuracy: 0.9490\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1709 - accuracy: 0.9488\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1667 - accuracy: 0.9499\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1689 - accuracy: 0.9495\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1673 - accuracy: 0.9505\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1695 - accuracy: 0.9503\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1694 - accuracy: 0.9506\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1668 - accuracy: 0.9512\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.1661 - accuracy: 0.9520\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "20096/60000 [=========>....................] - ETA: 1s - loss: 0.1654 - accuracy: 0.9522\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.1643 - accuracy: 0.9526\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.1651 - accuracy: 0.9521\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.1651 - accuracy: 0.9520\n26240/60000 [============>.................] - ETA: 1s - loss: 0.1656 - accuracy: 0.9515\n27776/60000 [============>.................] - ETA: 1s - loss: 0.1665 - accuracy: 0.9514\n29312/60000 [=============>................] - ETA: 1s - loss: 0.1651 - accuracy: 0.9516\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.1650 - accuracy: 0.9515\n32384/60000 [===============>..............] - ETA: 0s - loss: 0.1629 - accuracy: 0.9522\n33920/60000 [===============>..............] - ETA: 0s - loss: 0.1618 - accuracy: 0.9525\n35456/60000 [================>.............] - ETA: 0s - loss: 0.1617 - accuracy: 0.9524\n36992/60000 [=================>............] - ETA: 0s - loss: 0.1605 - accuracy: 0.9525\n38528/60000 [==================>...........] - ETA: 0s - loss: 0.1609 - accuracy: 0.9526\n40064/60000 [===================>..........] - ETA: 0s - loss: 0.1601 - accuracy: 0.9528\n41600/60000 [===================>..........] - ETA: 0s - loss: 0.1598 - accuracy: 0.9528\n43136/60000 [====================>.........] - ETA: 0s - loss: 0.1597 - accuracy: 0.9528\n44672/60000 [=====================>........] - ETA: 0s - loss: 0.1600 - accuracy: 0.9526\n46208/60000 [======================>.......] - ETA: 0s - loss: 0.1595 - accuracy: 0.9530\n47744/60000 [======================>.......] - ETA: 0s - loss: 0.1599 - accuracy: 0.9531\n49280/60000 [=======================>......] - ETA: 0s - loss: 0.1601 - accuracy: 0.9529\n50816/60000 [========================>.....] - ETA: 0s - loss: 0.1600 - accuracy: 0.9530\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.1600 - accuracy: 0.9530\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.1596 - accuracy: 0.9531\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.1590 - accuracy: 0.9533\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.1584 - accuracy: 0.9534\n58368/60000 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9535\n59904/60000 [============================>.] - ETA: 0s - loss: 0.1576 - accuracy: 0.9534\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1577 - accuracy: 0.9534 - val_loss: 0.0795 - val_accuracy: 0.9755\nEpoch 13/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9453\n 1664/60000 [..............................] - ETA: 2s - loss: 0.1504 - accuracy: 0.9549\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1461 - accuracy: 0.9597\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1445 - accuracy: 0.9595\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1442 - accuracy: 0.9600\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1497 - accuracy: 0.9599\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1499 - accuracy: 0.9582\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1539 - accuracy: 0.9560\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1535 - accuracy: 0.9560\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1547 - accuracy: 0.9556\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1540 - accuracy: 0.9560\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1511 - accuracy: 0.9568\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.1506 - accuracy: 0.9567\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.1504 - accuracy: 0.9565\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.1509 - accuracy: 0.9563\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.1520 - accuracy: 0.9562\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.1500 - accuracy: 0.9566\n26240/60000 [============>.................] - ETA: 1s - loss: 0.1502 - accuracy: 0.9569\n27776/60000 [============>.................] - ETA: 1s - loss: 0.1493 - accuracy: 0.9571\n29312/60000 [=============>................] - ETA: 1s - loss: 0.1488 - accuracy: 0.9574\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.1492 - accuracy: 0.9575\n32512/60000 [===============>..............] - ETA: 0s - loss: 0.1505 - accuracy: 0.9572\n34048/60000 [================>.............] - ETA: 0s - loss: 0.1507 - accuracy: 0.9571\n35584/60000 [================>.............] - ETA: 0s - loss: 0.1510 - accuracy: 0.9570\n37120/60000 [=================>............] - ETA: 0s - loss: 0.1492 - accuracy: 0.9575\n38656/60000 [==================>...........] - ETA: 0s - loss: 0.1496 - accuracy: 0.9569\n40192/60000 [===================>..........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9570\n41728/60000 [===================>..........] - ETA: 0s - loss: 0.1485 - accuracy: 0.9570\n43264/60000 [====================>.........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9567\n44800/60000 [=====================>........] - ETA: 0s - loss: 0.1491 - accuracy: 0.9567\n46336/60000 [======================>.......] - ETA: 0s - loss: 0.1487 - accuracy: 0.9570\n47872/60000 [======================>.......] - ETA: 0s - loss: 0.1484 - accuracy: 0.9572\n49408/60000 [=======================>......] - ETA: 0s - loss: 0.1478 - accuracy: 0.9573\n50944/60000 [========================>.....] - ETA: 0s - loss: 0.1479 - accuracy: 0.9575\n52480/60000 [=========================>....] - ETA: 0s - loss: 0.1475 - accuracy: 0.9574\n54016/60000 [==========================>...] - ETA: 0s - loss: 0.1482 - accuracy: 0.9574\n55552/60000 [==========================>...] - ETA: 0s - loss: 0.1482 - accuracy: 0.9573\n57088/60000 [===========================>..] - ETA: 0s - loss: 0.1479 - accuracy: 0.9575\n58624/60000 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9576\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1470 - accuracy: 0.9576 - val_loss: 0.0750 - val_accuracy: 0.9763\nEpoch 14/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1174 - accuracy: 0.9453\n 1664/60000 [..............................] - ETA: 2s - loss: 0.1339 - accuracy: 0.9585\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1425 - accuracy: 0.9563\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1444 - accuracy: 0.9542\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1418 - accuracy: 0.9555\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1422 - accuracy: 0.9550\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1395 - accuracy: 0.9564\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1388 - accuracy: 0.9576\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1390 - accuracy: 0.9579\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1410 - accuracy: 0.9572\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1392 - accuracy: 0.9576\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1403 - accuracy: 0.9577\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.1415 - accuracy: 0.9577\n19840/60000 [========>.....................] - ETA: 1s - loss: 0.1403 - accuracy: 0.9583\n21376/60000 [=========>....................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9594\n22912/60000 [==========>...................] - ETA: 1s - loss: 0.1379 - accuracy: 0.9593\n24448/60000 [===========>..................] - ETA: 1s - loss: 0.1386 - accuracy: 0.9592\n25984/60000 [===========>..................] - ETA: 1s - loss: 0.1401 - accuracy: 0.9591\n27392/60000 [============>.................] - ETA: 1s - loss: 0.1399 - accuracy: 0.9594\n28928/60000 [=============>................] - ETA: 1s - loss: 0.1389 - accuracy: 0.9597\n30464/60000 [==============>...............] - ETA: 0s - loss: 0.1385 - accuracy: 0.9597\n32000/60000 [===============>..............] - ETA: 0s - loss: 0.1378 - accuracy: 0.9599\n33536/60000 [===============>..............] - ETA: 0s - loss: 0.1372 - accuracy: 0.9599\n35072/60000 [================>.............] - ETA: 0s - loss: 0.1376 - accuracy: 0.9595\n36608/60000 [=================>............] - ETA: 0s - loss: 0.1379 - accuracy: 0.9594\n38144/60000 [==================>...........] - ETA: 0s - loss: 0.1376 - accuracy: 0.9594\n39680/60000 [==================>...........] - ETA: 0s - loss: 0.1364 - accuracy: 0.9596\n41216/60000 [===================>..........] - ETA: 0s - loss: 0.1369 - accuracy: 0.9595\n42752/60000 [====================>.........] - ETA: 0s - loss: 0.1375 - accuracy: 0.9593\n44288/60000 [=====================>........] - ETA: 0s - loss: 0.1382 - accuracy: 0.9593\n45824/60000 [=====================>........] - ETA: 0s - loss: 0.1386 - accuracy: 0.9592\n47360/60000 [======================>.......] - ETA: 0s - loss: 0.1382 - accuracy: 0.9593\n48896/60000 [=======================>......] - ETA: 0s - loss: 0.1378 - accuracy: 0.9593\n50304/60000 [========================>.....] - ETA: 0s - loss: 0.1379 - accuracy: 0.9592\n51840/60000 [========================>.....] - ETA: 0s - loss: 0.1386 - accuracy: 0.9590\n53376/60000 [=========================>....] - ETA: 0s - loss: 0.1377 - accuracy: 0.9593\n54912/60000 [==========================>...] - ETA: 0s - loss: 0.1380 - accuracy: 0.9592\n56448/60000 [===========================>..] - ETA: 0s - loss: 0.1381 - accuracy: 0.9593\n57984/60000 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.9592\n59520/60000 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9594\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1377 - accuracy: 0.9594 - val_loss: 0.0694 - val_accuracy: 0.9791\nEpoch 15/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.0435 - accuracy: 0.9922\n 1664/60000 [..............................] - ETA: 2s - loss: 0.1106 - accuracy: 0.9712\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1216 - accuracy: 0.9656\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1236 - accuracy: 0.9618\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": " 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1256 - accuracy: 0.9632\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1299 - accuracy: 0.9617\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1302 - accuracy: 0.9613\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1317 - accuracy: 0.9615\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1309 - accuracy: 0.9624\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1354 - accuracy: 0.9614\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1370 - accuracy: 0.9612\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1357 - accuracy: 0.9608\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.1365 - accuracy: 0.9605\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.1357 - accuracy: 0.9611\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.1363 - accuracy: 0.9614\n23168/60000 [==========>...................] - ETA: 1s - loss: 0.1361 - accuracy: 0.9617\n24704/60000 [===========>..................] - ETA: 1s - loss: 0.1356 - accuracy: 0.9620\n26240/60000 [============>.................] - ETA: 1s - loss: 0.1362 - accuracy: 0.9620\n27776/60000 [============>.................] - ETA: 1s - loss: 0.1359 - accuracy: 0.9624\n29312/60000 [=============>................] - ETA: 1s - loss: 0.1348 - accuracy: 0.9629\n30848/60000 [==============>...............] - ETA: 0s - loss: 0.1347 - accuracy: 0.9630\n32384/60000 [===============>..............] - ETA: 0s - loss: 0.1342 - accuracy: 0.9628\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.1341 - accuracy: 0.9630\n35328/60000 [================>.............] - ETA: 0s - loss: 0.1345 - accuracy: 0.9628\n36864/60000 [=================>............] - ETA: 0s - loss: 0.1347 - accuracy: 0.9628\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.1349 - accuracy: 0.9628\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.1334 - accuracy: 0.9629\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.1325 - accuracy: 0.9631\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.1341 - accuracy: 0.9628\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.1344 - accuracy: 0.9628\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.1335 - accuracy: 0.9630\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.1336 - accuracy: 0.9628\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.1329 - accuracy: 0.9629\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9629\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.1320 - accuracy: 0.9629\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.1316 - accuracy: 0.9629\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.1316 - accuracy: 0.9629\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.1316 - accuracy: 0.9627\n58368/60000 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9627\n59904/60000 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9628\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1314 - accuracy: 0.9628 - val_loss: 0.0719 - val_accuracy: 0.9797\nEpoch 16/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.1963 - accuracy: 0.9609\n 1664/60000 [..............................] - ETA: 2s - loss: 0.1316 - accuracy: 0.9567\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1388 - accuracy: 0.9597\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1299 - accuracy: 0.9626\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1313 - accuracy: 0.9616\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1336 - accuracy: 0.9604\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1307 - accuracy: 0.9619\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1303 - accuracy: 0.9620\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1309 - accuracy: 0.9625\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1307 - accuracy: 0.9623\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1335 - accuracy: 0.9616\n16896/60000 [=======>......................] - ETA: 1s - loss: 0.1346 - accuracy: 0.9616\n18432/60000 [========>.....................] - ETA: 1s - loss: 0.1327 - accuracy: 0.9624\n19968/60000 [========>.....................] - ETA: 1s - loss: 0.1306 - accuracy: 0.9628\n21504/60000 [=========>....................] - ETA: 1s - loss: 0.1314 - accuracy: 0.9628\n23040/60000 [==========>...................] - ETA: 1s - loss: 0.1303 - accuracy: 0.9630\n24576/60000 [===========>..................] - ETA: 1s - loss: 0.1312 - accuracy: 0.9630\n26112/60000 [============>.................] - ETA: 1s - loss: 0.1302 - accuracy: 0.9633\n27648/60000 [============>.................] - ETA: 1s - loss: 0.1288 - accuracy: 0.9637\n29056/60000 [=============>................] - ETA: 1s - loss: 0.1281 - accuracy: 0.9638\n30592/60000 [==============>...............] - ETA: 1s - loss: 0.1274 - accuracy: 0.9640\n32128/60000 [===============>..............] - ETA: 0s - loss: 0.1264 - accuracy: 0.9644\n33664/60000 [===============>..............] - ETA: 0s - loss: 0.1268 - accuracy: 0.9641\n35200/60000 [================>.............] - ETA: 0s - loss: 0.1261 - accuracy: 0.9645\n36736/60000 [=================>............] - ETA: 0s - loss: 0.1252 - accuracy: 0.9649\n38272/60000 [==================>...........] - ETA: 0s - loss: 0.1245 - accuracy: 0.9647\n39808/60000 [==================>...........] - ETA: 0s - loss: 0.1238 - accuracy: 0.9650\n41344/60000 [===================>..........] - ETA: 0s - loss: 0.1236 - accuracy: 0.9650\n42880/60000 [====================>.........] - ETA: 0s - loss: 0.1229 - accuracy: 0.9649\n44416/60000 [=====================>........] - ETA: 0s - loss: 0.1228 - accuracy: 0.9651\n45952/60000 [=====================>........] - ETA: 0s - loss: 0.1230 - accuracy: 0.9650\n47488/60000 [======================>.......] - ETA: 0s - loss: 0.1224 - accuracy: 0.9651\n49024/60000 [=======================>......] - ETA: 0s - loss: 0.1225 - accuracy: 0.9651\n50560/60000 [========================>.....] - ETA: 0s - loss: 0.1224 - accuracy: 0.9652\n52096/60000 [=========================>....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9649\n53632/60000 [=========================>....] - ETA: 0s - loss: 0.1234 - accuracy: 0.9649\n55168/60000 [==========================>...] - ETA: 0s - loss: 0.1229 - accuracy: 0.9651\n56704/60000 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.9651\n58240/60000 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9652\n59776/60000 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9652\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1230 - accuracy: 0.9652 - val_loss: 0.0603 - val_accuracy: 0.9807\nEpoch 17/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.0691 - accuracy: 0.9844\n 1664/60000 [..............................] - ETA: 2s - loss: 0.0982 - accuracy: 0.9754\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1069 - accuracy: 0.9700\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1078 - accuracy: 0.9690\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1091 - accuracy: 0.9673\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1108 - accuracy: 0.9679\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1109 - accuracy: 0.9675\n10880/60000 [====>.........................] - ETA: 1s - loss: 0.1086 - accuracy: 0.9671\n12416/60000 [=====>........................] - ETA: 1s - loss: 0.1091 - accuracy: 0.9674\n13824/60000 [=====>........................] - ETA: 1s - loss: 0.1092 - accuracy: 0.9667\n15360/60000 [======>.......................] - ETA: 1s - loss: 0.1102 - accuracy: 0.9663\n16896/60000 [=======>......................] - ETA: 1s - loss: 0.1119 - accuracy: 0.9665\n18432/60000 [========>.....................] - ETA: 1s - loss: 0.1112 - accuracy: 0.9669\n19968/60000 [========>.....................] - ETA: 1s - loss: 0.1120 - accuracy: 0.9672\n21504/60000 [=========>....................] - ETA: 1s - loss: 0.1110 - accuracy: 0.9676\n23040/60000 [==========>...................] - ETA: 1s - loss: 0.1112 - accuracy: 0.9678\n24576/60000 [===========>..................] - ETA: 1s - loss: 0.1114 - accuracy: 0.9680\n26112/60000 [============>.................] - ETA: 1s - loss: 0.1109 - accuracy: 0.9685\n27648/60000 [============>.................] - ETA: 1s - loss: 0.1126 - accuracy: 0.9680\n29184/60000 [=============>................] - ETA: 1s - loss: 0.1128 - accuracy: 0.9679\n30720/60000 [==============>...............] - ETA: 0s - loss: 0.1139 - accuracy: 0.9677\n32256/60000 [===============>..............] - ETA: 0s - loss: 0.1152 - accuracy: 0.9673\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.1149 - accuracy: 0.9674\n35328/60000 [================>.............] - ETA: 0s - loss: 0.1151 - accuracy: 0.9671\n36864/60000 [=================>............] - ETA: 0s - loss: 0.1152 - accuracy: 0.9670\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.1145 - accuracy: 0.9673\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.1145 - accuracy: 0.9670\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.1141 - accuracy: 0.9671\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.1144 - accuracy: 0.9671\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.1144 - accuracy: 0.9674\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.1149 - accuracy: 0.9672\n47360/60000 [======================>.......] - ETA: 0s - loss: 0.1148 - accuracy: 0.9672\n48896/60000 [=======================>......] - ETA: 0s - loss: 0.1153 - accuracy: 0.9671\n50432/60000 [========================>.....] - ETA: 0s - loss: 0.1162 - accuracy: 0.9669\n51968/60000 [========================>.....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9670\n53504/60000 [=========================>....] - ETA: 0s - loss: 0.1167 - accuracy: 0.9668\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "55040/60000 [==========================>...] - ETA: 0s - loss: 0.1165 - accuracy: 0.9668\n56576/60000 [===========================>..] - ETA: 0s - loss: 0.1167 - accuracy: 0.9669\n58112/60000 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9670\n59648/60000 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9671\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1156 - accuracy: 0.9672 - val_loss: 0.0636 - val_accuracy: 0.9813\nEpoch 18/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.0936 - accuracy: 0.9766\n 1664/60000 [..............................] - ETA: 2s - loss: 0.1031 - accuracy: 0.9742\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1096 - accuracy: 0.9716\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1085 - accuracy: 0.9709\n 6144/60000 [==>...........................] - ETA: 1s - loss: 0.1105 - accuracy: 0.9697\n 7680/60000 [==>...........................] - ETA: 1s - loss: 0.1155 - accuracy: 0.9678\n 9216/60000 [===>..........................] - ETA: 1s - loss: 0.1178 - accuracy: 0.9669\n10752/60000 [====>.........................] - ETA: 1s - loss: 0.1171 - accuracy: 0.9666\n12288/60000 [=====>........................] - ETA: 1s - loss: 0.1150 - accuracy: 0.9674\n13824/60000 [=====>........................] - ETA: 1s - loss: 0.1146 - accuracy: 0.9672\n15360/60000 [======>.......................] - ETA: 1s - loss: 0.1183 - accuracy: 0.9663\n16896/60000 [=======>......................] - ETA: 1s - loss: 0.1171 - accuracy: 0.9664\n18432/60000 [========>.....................] - ETA: 1s - loss: 0.1170 - accuracy: 0.9663\n19968/60000 [========>.....................] - ETA: 1s - loss: 0.1159 - accuracy: 0.9666\n21504/60000 [=========>....................] - ETA: 1s - loss: 0.1163 - accuracy: 0.9665\n23040/60000 [==========>...................] - ETA: 1s - loss: 0.1156 - accuracy: 0.9665\n24576/60000 [===========>..................] - ETA: 1s - loss: 0.1140 - accuracy: 0.9670\n26112/60000 [============>.................] - ETA: 1s - loss: 0.1131 - accuracy: 0.9675\n27648/60000 [============>.................] - ETA: 1s - loss: 0.1138 - accuracy: 0.9678\n29184/60000 [=============>................] - ETA: 1s - loss: 0.1128 - accuracy: 0.9679\n30720/60000 [==============>...............] - ETA: 0s - loss: 0.1123 - accuracy: 0.9678\n32256/60000 [===============>..............] - ETA: 0s - loss: 0.1117 - accuracy: 0.9677\n33792/60000 [===============>..............] - ETA: 0s - loss: 0.1117 - accuracy: 0.9677\n35328/60000 [================>.............] - ETA: 0s - loss: 0.1109 - accuracy: 0.9678\n36864/60000 [=================>............] - ETA: 0s - loss: 0.1105 - accuracy: 0.9680\n38400/60000 [==================>...........] - ETA: 0s - loss: 0.1107 - accuracy: 0.9681\n39936/60000 [==================>...........] - ETA: 0s - loss: 0.1106 - accuracy: 0.9681\n41472/60000 [===================>..........] - ETA: 0s - loss: 0.1116 - accuracy: 0.9679\n43008/60000 [====================>.........] - ETA: 0s - loss: 0.1117 - accuracy: 0.9678\n44544/60000 [=====================>........] - ETA: 0s - loss: 0.1124 - accuracy: 0.9679\n46080/60000 [======================>.......] - ETA: 0s - loss: 0.1129 - accuracy: 0.9677\n47616/60000 [======================>.......] - ETA: 0s - loss: 0.1130 - accuracy: 0.9676\n49152/60000 [=======================>......] - ETA: 0s - loss: 0.1127 - accuracy: 0.9675\n50688/60000 [========================>.....] - ETA: 0s - loss: 0.1136 - accuracy: 0.9672\n52224/60000 [=========================>....] - ETA: 0s - loss: 0.1132 - accuracy: 0.9672\n53760/60000 [=========================>....] - ETA: 0s - loss: 0.1128 - accuracy: 0.9674\n55296/60000 [==========================>...] - ETA: 0s - loss: 0.1120 - accuracy: 0.9676\n56832/60000 [===========================>..] - ETA: 0s - loss: 0.1116 - accuracy: 0.9678\n58368/60000 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9677\n59904/60000 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9677\n60000/60000 [==============================] - 2s 36us/step - loss: 0.1122 - accuracy: 0.9677 - val_loss: 0.0557 - val_accuracy: 0.9826\nEpoch 19/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.0924 - accuracy: 0.9844\n 1536/60000 [..............................] - ETA: 2s - loss: 0.0973 - accuracy: 0.9746\n 3072/60000 [>.............................] - ETA: 2s - loss: 0.0927 - accuracy: 0.9736\n 4608/60000 [=>............................] - ETA: 1s - loss: 0.1002 - accuracy: 0.9737\n 6144/60000 [==>...........................] - ETA: 1s - loss: 0.1034 - accuracy: 0.9730\n 7680/60000 [==>...........................] - ETA: 1s - loss: 0.1062 - accuracy: 0.9717\n 9216/60000 [===>..........................] - ETA: 1s - loss: 0.1020 - accuracy: 0.9728\n10752/60000 [====>.........................] - ETA: 1s - loss: 0.1010 - accuracy: 0.9729\n12288/60000 [=====>........................] - ETA: 1s - loss: 0.1035 - accuracy: 0.9716\n13824/60000 [=====>........................] - ETA: 1s - loss: 0.1033 - accuracy: 0.9716\n15360/60000 [======>.......................] - ETA: 1s - loss: 0.1022 - accuracy: 0.9718\n16896/60000 [=======>......................] - ETA: 1s - loss: 0.1027 - accuracy: 0.9717\n18432/60000 [========>.....................] - ETA: 1s - loss: 0.1043 - accuracy: 0.9706\n19968/60000 [========>.....................] - ETA: 1s - loss: 0.1040 - accuracy: 0.9711\n21504/60000 [=========>....................] - ETA: 1s - loss: 0.1041 - accuracy: 0.9708\n23040/60000 [==========>...................] - ETA: 1s - loss: 0.1039 - accuracy: 0.9706\n24576/60000 [===========>..................] - ETA: 1s - loss: 0.1051 - accuracy: 0.9699\n26112/60000 [============>.................] - ETA: 1s - loss: 0.1040 - accuracy: 0.9703\n27648/60000 [============>.................] - ETA: 1s - loss: 0.1045 - accuracy: 0.9703\n29056/60000 [=============>................] - ETA: 1s - loss: 0.1052 - accuracy: 0.9701\n30592/60000 [==============>...............] - ETA: 1s - loss: 0.1048 - accuracy: 0.9702\n32128/60000 [===============>..............] - ETA: 0s - loss: 0.1059 - accuracy: 0.9696\n33664/60000 [===============>..............] - ETA: 0s - loss: 0.1068 - accuracy: 0.9695\n35200/60000 [================>.............] - ETA: 0s - loss: 0.1070 - accuracy: 0.9696\n36736/60000 [=================>............] - ETA: 0s - loss: 0.1078 - accuracy: 0.9696\n38272/60000 [==================>...........] - ETA: 0s - loss: 0.1079 - accuracy: 0.9696\n39680/60000 [==================>...........] - ETA: 0s - loss: 0.1078 - accuracy: 0.9695\n41216/60000 [===================>..........] - ETA: 0s - loss: 0.1075 - accuracy: 0.9696\n42752/60000 [====================>.........] - ETA: 0s - loss: 0.1067 - accuracy: 0.9697\n44288/60000 [=====================>........] - ETA: 0s - loss: 0.1070 - accuracy: 0.9694\n45824/60000 [=====================>........] - ETA: 0s - loss: 0.1069 - accuracy: 0.9695\n47360/60000 [======================>.......] - ETA: 0s - loss: 0.1069 - accuracy: 0.9694\n48896/60000 [=======================>......] - ETA: 0s - loss: 0.1069 - accuracy: 0.9695\n50432/60000 [========================>.....] - ETA: 0s - loss: 0.1059 - accuracy: 0.9696\n51968/60000 [========================>.....] - ETA: 0s - loss: 0.1062 - accuracy: 0.9698\n53504/60000 [=========================>....] - ETA: 0s - loss: 0.1055 - accuracy: 0.9700\n55040/60000 [==========================>...] - ETA: 0s - loss: 0.1054 - accuracy: 0.9699\n56576/60000 [===========================>..] - ETA: 0s - loss: 0.1059 - accuracy: 0.9698\n58112/60000 [============================>.] - ETA: 0s - loss: 0.1063 - accuracy: 0.9698\n59648/60000 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9698\n60000/60000 [==============================] - 2s 37us/step - loss: 0.1061 - accuracy: 0.9699 - val_loss: 0.0579 - val_accuracy: 0.9825\nEpoch 20/20\n\n  128/60000 [..............................] - ETA: 2s - loss: 0.0976 - accuracy: 0.9688\n 1664/60000 [..............................] - ETA: 2s - loss: 0.0938 - accuracy: 0.9742\n 3200/60000 [>.............................] - ETA: 1s - loss: 0.1190 - accuracy: 0.9669\n 4736/60000 [=>............................] - ETA: 1s - loss: 0.1082 - accuracy: 0.9692\n 6272/60000 [==>...........................] - ETA: 1s - loss: 0.1053 - accuracy: 0.9680\n 7808/60000 [==>...........................] - ETA: 1s - loss: 0.1053 - accuracy: 0.9693\n 9344/60000 [===>..........................] - ETA: 1s - loss: 0.1102 - accuracy: 0.9671\n11008/60000 [====>.........................] - ETA: 1s - loss: 0.1126 - accuracy: 0.9672\n12544/60000 [=====>........................] - ETA: 1s - loss: 0.1120 - accuracy: 0.9676\n13952/60000 [=====>........................] - ETA: 1s - loss: 0.1105 - accuracy: 0.9688\n15488/60000 [======>.......................] - ETA: 1s - loss: 0.1099 - accuracy: 0.9687\n17024/60000 [=======>......................] - ETA: 1s - loss: 0.1079 - accuracy: 0.9693\n18560/60000 [========>.....................] - ETA: 1s - loss: 0.1098 - accuracy: 0.9691\n20096/60000 [=========>....................] - ETA: 1s - loss: 0.1078 - accuracy: 0.9696\n21632/60000 [=========>....................] - ETA: 1s - loss: 0.1062 - accuracy: 0.9702\n22912/60000 [==========>...................] - ETA: 1s - loss: 0.1065 - accuracy: 0.9702\n24448/60000 [===========>..................] - ETA: 1s - loss: 0.1067 - accuracy: 0.9703\n25856/60000 [===========>..................] - ETA: 1s - loss: 0.1060 - accuracy: 0.9705\n27392/60000 [============>.................] - ETA: 1s - loss: 0.1053 - accuracy: 0.9706\n28928/60000 [=============>................] - ETA: 1s - loss: 0.1045 - accuracy: 0.9705\n30464/60000 [==============>...............] - ETA: 1s - loss: 0.1037 - accuracy: 0.9709\n32000/60000 [===============>..............] - ETA: 0s - loss: 0.1042 - accuracy: 0.9709\n33536/60000 [===============>..............] - ETA: 0s - loss: 0.1045 - accuracy: 0.9706\n35072/60000 [================>.............] - ETA: 0s - loss: 0.1041 - accuracy: 0.9707\n36608/60000 [=================>............] - ETA: 0s - loss: 0.1040 - accuracy: 0.9708\n38144/60000 [==================>...........] - ETA: 0s - loss: 0.1041 - accuracy: 0.9707\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "39680/60000 [==================>...........] - ETA: 0s - loss: 0.1050 - accuracy: 0.9706\n41216/60000 [===================>..........] - ETA: 0s - loss: 0.1043 - accuracy: 0.9707\n42752/60000 [====================>.........] - ETA: 0s - loss: 0.1043 - accuracy: 0.9706\n44288/60000 [=====================>........] - ETA: 0s - loss: 0.1042 - accuracy: 0.9705\n45824/60000 [=====================>........] - ETA: 0s - loss: 0.1050 - accuracy: 0.9704\n47360/60000 [======================>.......] - ETA: 0s - loss: 0.1044 - accuracy: 0.9706\n48896/60000 [=======================>......] - ETA: 0s - loss: 0.1056 - accuracy: 0.9702\n50432/60000 [========================>.....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9702\n51968/60000 [========================>.....] - ETA: 0s - loss: 0.1052 - accuracy: 0.9704\n53504/60000 [=========================>....] - ETA: 0s - loss: 0.1045 - accuracy: 0.9705\n55040/60000 [==========================>...] - ETA: 0s - loss: 0.1045 - accuracy: 0.9703\n56576/60000 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9704\n58112/60000 [============================>.] - ETA: 0s - loss: 0.1039 - accuracy: 0.9704\n59648/60000 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9705\n60000/60000 [==============================] - 2s 37us/step - loss: 0.1035 - accuracy: 0.9704 - val_loss: 0.0595 - val_accuracy: 0.9813\nTest loss: 0.05951368046371499\nTest accuracy: 0.9812999963760376\nmodel saved in ./outputs/model folder\n\n\nThe experiment completed successfully. Finalizing run...\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.33400511741638184 seconds\n\nStreaming azureml-logs/75_job_post-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt\n===============================================================================================================\n\nbash: /azureml-envs/azureml_afeee008b9fa94f87e942e78a75569be/lib/libtinfo.so.5: no version information available (required by bash)\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Exception in thread Thread-12:\nTraceback (most recent call last):\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/multiprocessing/pool.py\", line 479, in _handle_results\n    cache[job]._set(i, obj)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/multiprocessing/pool.py\", line 649, in _set\n    self._callback(self._value)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/azureml/widgets/_userrun/_run_details.py\", line 503, in _update_metrics\n    self.widget_instance.run_metrics = result\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/traitlets/traitlets.py\", line 585, in __set__\n    self.set(obj, value)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/traitlets/traitlets.py\", line 574, in set\n    obj._notify_trait(self.name, old_value, new_value)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/traitlets/traitlets.py\", line 1139, in _notify_trait\n    type='change',\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipywidgets/widgets/widget.py\", line 599, in notify_change\n    self.send_state(key=name)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipywidgets/widgets/widget.py\", line 484, in send_state\n    self._send(msg, buffers=buffers)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipywidgets/widgets/widget.py\", line 729, in _send\n    self.comm.send(data=msg, buffers=buffers)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/comm/comm.py\", line 121, in send\n    data=data, metadata=metadata, buffers=buffers,\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/comm/comm.py\", line 65, in _publish_msg\n    content = json_clean(dict(data=data, comm_id=self.comm_id, **keys))\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 191, in json_clean\n    out[unicode_type(k)] = json_clean(v)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 191, in json_clean\n    out[unicode_type(k)] = json_clean(v)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 191, in json_clean\n    out[unicode_type(k)] = json_clean(v)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 177, in json_clean\n    return [json_clean(x) for x in obj]\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 177, in <listcomp>\n    return [json_clean(x) for x in obj]\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 191, in json_clean\n    out[unicode_type(k)] = json_clean(v)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 177, in json_clean\n    return [json_clean(x) for x in obj]\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 177, in <listcomp>\n    return [json_clean(x) for x in obj]\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 191, in json_clean\n    out[unicode_type(k)] = json_clean(v)\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 177, in json_clean\n    return [json_clean(x) for x in obj]\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 177, in <listcomp>\n    return [json_clean(x) for x in obj]\n  File \"/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/jsonutil.py\", line 197, in json_clean\n    raise ValueError(\"Can't clean for JSON: %r\" % obj)\nValueError: Can't clean for JSON: Artifact(data_location=aml://artifactId/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/Accuracy vs Loss_1570403565.png, filename=Accuracy vs Loss_1570403565.png, metric_type=azureml.v2.image)\n\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Starting job release. Current time:2019-10-06T23:12:50.531687\nLogging experiment finalizing status in history service.\nStarting the daemon thread to refresh tokens in background for process with pid = 343\nJob release is complete. Current time:2019-10-06T23:12:52.858599\n\nExecution Summary\n=================\nRunId: keras-mnist_1570403228_cf52f5a7\nWeb View: https://mlworkspace.azure.ai/portal/subscriptions/ca28e367-312d-4b07-a3fd-650390469a4e/resourceGroups/dahatakemlus/providers/Microsoft.MachineLearningServices/workspaces/dahatakemlus/experiments/keras-mnist/runs/keras-mnist_1570403228_cf52f5a7\n\nCPU times: user 16.8 s, sys: 1.65 s, total: 18.4 s\nWall time: 5min 22s\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "{'runId': 'keras-mnist_1570403228_cf52f5a7',\n 'target': 'gpu-cluster',\n 'status': 'Completed',\n 'startTimeUtc': '2019-10-06T23:09:46.071197Z',\n 'endTimeUtc': '2019-10-06T23:13:04.600759Z',\n 'properties': {'_azureml.ComputeTargetType': 'batchai',\n  'ContentSnapshotId': 'a6a44e2b-e3cb-4c8c-8063-a2e77c1cce47',\n  'AzureML.DerivedImageName': 'azureml/azureml_d27c0495535dfab8cde5924dd9292416',\n  'ProcessInfoFile': 'azureml-logs/process_info.json',\n  'ProcessStatusFile': 'azureml-logs/process_status.json',\n  'azureml.git.repository_uri': 'https://github.com/dahatake/AzureMachineLearningServices-HyperparameterTurning-sample',\n  'mlflow.source.git.repoURL': 'https://github.com/dahatake/AzureMachineLearningServices-HyperparameterTurning-sample',\n  'azureml.git.branch': 'master',\n  'mlflow.source.git.branch': 'master',\n  'azureml.git.commit': '6aa0b504614817af13bcbcfa84f9ff051f58d1aa',\n  'mlflow.source.git.commit': '6aa0b504614817af13bcbcfa84f9ff051f58d1aa',\n  'azureml.git.dirty': 'True'},\n 'runDefinition': {'script': 'mnist_cnn.py',\n  'arguments': ['--data-folder',\n   '$AZUREML_DATAREFERENCE_4d6e15e054e3489db3866f5c7ac58958',\n   '--batch-size',\n   '128',\n   '--epoch',\n   '20',\n   '--neurons-1',\n   '32',\n   '--neurons-2',\n   '64',\n   '--neurons-3',\n   '128',\n   '--kernel-size-1',\n   '3',\n   '--kernel-size-2',\n   '3',\n   '--pool-size',\n   '2',\n   '--learning-rate',\n   '0.001',\n   '--activation',\n   'relu',\n   '--optimizer',\n   'RMSprop',\n   '--loss',\n   'categorical_crossentropy',\n   '--dropout-1',\n   '0.2',\n   '--dropout-2',\n   '0.5',\n   '--gpu',\n   '1',\n   '--auto_mixed_precision',\n   '1'],\n  'sourceDirectoryDataStore': None,\n  'framework': 'Python',\n  'communicator': 'None',\n  'target': 'gpu-cluster',\n  'dataReferences': {'4d6e15e054e3489db3866f5c7ac58958': {'dataStoreName': 'workspaceblobstore',\n    'mode': 'Mount',\n    'pathOnDataStore': 'mnist',\n    'pathOnCompute': None,\n    'overwrite': False}},\n  'data': {},\n  'jobName': None,\n  'maxRunDurationSeconds': 600,\n  'nodeCount': 1,\n  'environment': {'name': 'Experiment keras-mnist Environment',\n   'version': 'Autosave_2019-10-04T10:47:15Z_0e277823',\n   'python': {'interpreterPath': 'python',\n    'userManagedDependencies': False,\n    'condaDependencies': {'channels': ['conda-forge'],\n     'dependencies': ['python=3.6.2',\n      {'pip': ['keras',\n        'matplotlib',\n        'azureml-defaults',\n        'tensorflow-gpu==1.13.1',\n        'horovod==0.16.1']}],\n     'name': 'azureml_afeee008b9fa94f87e942e78a75569be'},\n    'baseCondaEnvironment': None},\n   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04',\n    'baseDockerfile': None,\n    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n    'enabled': True,\n    'arguments': []},\n   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n   'inferencingStackVersion': None},\n  'history': {'outputCollection': True,\n   'directoriesToWatch': ['logs'],\n   'snapshotProject': True},\n  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'amlCompute': {'name': None,\n   'vmSize': None,\n   'retainCluster': False,\n   'clusterMaxNodeCount': 1},\n  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n  'mpi': {'processCountPerNode': 1},\n  'hdi': {'yarnDeployMode': 'Cluster'},\n  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n  'exposedPorts': None,\n  'docker': {'useDocker': True,\n   'sharedVolumes': True,\n   'shmSize': '2g',\n   'arguments': []}},\n 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/azureml-logs/55_azureml-execution-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt?sv=2018-11-09&sr=b&sig=Cofg05BQjL0FD7fjqcpPzfWtkQimGfyR23P59RoWC04%3D&st=2019-10-06T23%3A03%3A05Z&se=2019-10-07T07%3A13%3A05Z&sp=r',\n  'azureml-logs/65_job_prep-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/azureml-logs/65_job_prep-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt?sv=2018-11-09&sr=b&sig=SwM7YZUdOEbPmISpJsNFomrf8Yw2E%2FpsBI9DI8HnsFM%3D&st=2019-10-06T23%3A03%3A05Z&se=2019-10-07T07%3A13%3A05Z&sp=r',\n  'azureml-logs/70_driver_log.txt': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/azureml-logs/70_driver_log.txt?sv=2018-11-09&sr=b&sig=1SHf%2BIw%2Bse5UpULDOG5QErc%2BgD1%2Fx19ZZwKbMvmDc8s%3D&st=2019-10-06T23%3A03%3A05Z&se=2019-10-07T07%3A13%3A05Z&sp=r',\n  'azureml-logs/75_job_post-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/azureml-logs/75_job_post-tvmps_0fcbe7b7f737e1cdc378a9b02c96d4c0e6daedf242ddce2df1258c209d0d65cc_p.txt?sv=2018-11-09&sr=b&sig=wrMo5McBqdQ6piDCjmFglDEkENH7NNISpO6bfurURbE%3D&st=2019-10-06T23%3A03%3A05Z&se=2019-10-07T07%3A13%3A05Z&sp=r',\n  'logs/azureml/139_azureml.log': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/logs/azureml/139_azureml.log?sv=2018-11-09&sr=b&sig=8kl8FmQ6I9TVghtZWONW%2BrGIvEcIFjMu%2B5te5aluLL4%3D&st=2019-10-06T23%3A03%3A05Z&se=2019-10-07T07%3A13%3A05Z&sp=r',\n  'logs/azureml/azureml.log': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570403228_cf52f5a7/logs/azureml/azureml.log?sv=2018-11-09&sr=b&sig=bc4LzH2lorA7fPZ%2FVG0dWh%2F72Zp1yK3X4afMBevEAQY%3D&st=2019-10-06T23%3A03%3A05Z&se=2019-10-07T07%3A13%3A05Z&sp=r'}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the outputs of the training script, it prints out the Keras version number. Please make a note of it."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The Run object\nThe Run object provides the interface to the run history -- both to the job and to the control plane (this notebook), and both while the job is running and after it has completed. It provides a number of interesting features for instance:\n* `run.get_details()`: Provides a rich set of properties of the run\n* `run.get_metrics()`: Provides a dictionary with all the metrics that were reported for the Run\n* `run.get_file_names()`: List all the files that were uploaded to the run history for this Run. This will include the `outputs` and `logs` folder, azureml-logs and other logs, as well as files that were explicitly uploaded to the run using `run.upload_file()`\n\nBelow are some examples -- please run through them and inspect their output. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#run.get_metrics()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Download the saved model"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the training script, the Keras model is saved into two files, `model.json` and `model.h5`, in the `outputs/models` folder on the gpucluster AmlCompute node. Azure ML automatically uploaded anything written in the `./outputs` folder into run history file store. Subsequently, we can use the `run` object to download the model files. They are under the the `outputs/model` folder in the run history file store, and are downloaded into a local folder named `model`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# create a model folder in the current directory\nos.makedirs('./model', exist_ok=True)\n\nfor f in run.get_file_names():\n    if f.startswith('outputs/model'):\n        output_file_path = os.path.join('./model', f.split('/')[-1])\n        print('Downloading from {} to {} ...'.format(f, output_file_path))\n        run.download_file(name=f, output_file_path=output_file_path)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading from outputs/model/mnist.h5 to ./model/mnist.h5 ...\nDownloading from outputs/model/mnist.json to ./model/mnist.json ...\nDownloading from outputs/model/mnist_weights.h5 to ./model/mnist_weights.h5 ...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's load the downloaded model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import load_model\nfrom keras.models import model_from_json\n\n# load model file\nmodel = load_model('model/mnist.h5')\nprint(\"Model loaded from disk.\")",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model loaded from disk.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import keras\nimport tensorflow as tf\n\nprint(\"Keras version:\", keras.__version__)\nprint(\"Tensorflow version:\", tf.__version__)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Keras version: 2.2.4\nTensorflow version: 1.12.2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## debug for check existing MNIST onnx file from WindowsML sample structure via python\n\n#import onnx\n\n#model =  onnx.load('model/mnist_cs.onnx')\n#print(model.graph.input[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Convert model file to ONNX\nSupported ONNX opset version is 7.\nhttps://docs.microsoft.com/ja-jp/windows/ai/windows-ml/onnx-versions\n\nreference: https://github.com/onnx/onnxmltools and https://github.com/onnx/keras-onnx/"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import onnxmltools\n\ndef convert_onnx(model, filename):\n    onnx_model = onnxmltools.convert_keras(model, target_opset=7)\n\n    batch_size=1\n    onnx_model.graph.input[0].type.tensor_type.shape.dim[0].dim_value = batch_size\n    onnx_model.graph.input[0].type.tensor_type.shape.dim[0].denotation = 'DATA_BATCH'\n    onnx_model.graph.input[0].type.tensor_type.shape.dim[1].denotation = 'DATA_CHANNEL'\n    onnx_model.graph.input[0].type.tensor_type.shape.dim[2].denotation = 'DATA_FEATURE'\n    onnx_model.graph.input[0].type.tensor_type.shape.dim[3].denotation = 'DATA_FEATURE'\n    onnx_model.graph.input[0].type.denotation = 'IMAGE'\n    \n    print(onnx_model.graph.input[0])\n    \n    onnxmltools.utils.save_model(onnx_model, filename)\n\nconvert_onnx(model, 'model/mnist.onnx')\nprint('onnx file saved')",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": "name: \"conv2d_1_input\"\ntype {\n  tensor_type {\n    elem_type: 1\n    shape {\n      dim {\n        dim_value: 1\n        denotation: \"DATA_BATCH\"\n      }\n      dim {\n        dim_value: 1\n        denotation: \"DATA_CHANNEL\"\n      }\n      dim {\n        dim_value: 28\n        denotation: \"DATA_FEATURE\"\n      }\n      dim {\n        dim_value: 28\n        denotation: \"DATA_FEATURE\"\n      }\n    }\n  }\n  denotation: \"IMAGE\"\n}\n\nonnx file saved\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2. Intelligent hyperparameter tuning\nWe have trained the model with one set of hyperparameters, now let's how we can do hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import BayesianParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice, loguniform, uniform\n\n# BayesianParameterSampling dones't support Eearly Termination Policy\n# https://docs.microsoft.com/ja-jp/azure/machine-learning/service/how-to-tune-hyperparameters\n\nps = BayesianParameterSampling(\n    {\n        '--batch-size': choice(50, 100, 150, 200, 250, 300, 350, 400),\n        '--epoch': choice(20, 25, 30, 35),\n        '--neurons-1': choice(16, 32, 64, 128),\n        '--neurons-2': choice(16, 32, 64, 128, 256),\n        '--neurons-3': choice(16, 32, 64, 128, 256, 512),\n        '--kernel-size-1': choice(2, 3, 4, 5, 6, 7),\n        '--kernel-size-2': choice(2, 3, 4, 5, 6, 7),\n        '--pool-size': choice(2, 3, 4, 5, 6, 7),\n        '--learning-rate': uniform(0.0001, 0.1),\n        '--activation': choice('softmax','elu','selu','softplus','softsign','relu','tanh','sigmoid','hard_sigmoid'),\n        '--optimizer': choice('SGD','RMSprop','Adagrad','Adadelta','Adam','Adamax','Nadam'),\n        '--dropout-1': uniform(0.1, 0.5),\n        '--dropout-2': uniform(0.1, 0.5)\n    }\n)",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we will create a new estimator without the above parameters since they will be passed in later by Hyperdrive configuration. Note we still need to keep the `data-folder` parameter since that's not a hyperparamter we will sweep."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "est = TensorFlow(source_directory=script_folder,\n                 script_params={'--data-folder': ds.path('mnist').as_mount(),\n                                '--loss': 'categorical_crossentropy',\n                                '--gpu': gpu_count,\n                                '--auto_mixed_precision': tensorcore_status\n                               },\n                 compute_target=compute_target,\n                 conda_packages=['keras', 'matplotlib'],\n                 entry_script='mnist_cnn.py', \n                 use_gpu=True)",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": "WARNING - framework_version is not specified, defaulting to version 1.13.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we are ready to configure a run configuration object, and specify the primary metric `Accuracy` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hdc = HyperDriveConfig(estimator=est, \n                       hyperparameter_sampling=ps, \n                       primary_metric_name='Accuracy', \n                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                       max_total_runs=260,\n                       max_concurrent_runs=4,\n                       max_duration_minutes=20)",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, let's launch the hyperparameter tuning job."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hdr = exp.submit(config=hdc)",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(hdr).show()",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aa4102649644bb38efe3c5c7ae6b37c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hdr.wait_for_completion(show_output=True)",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "RunId: keras-mnist_1570404439206766\nWeb View: https://mlworkspace.azure.ai/portal/subscriptions/ca28e367-312d-4b07-a3fd-650390469a4e/resourceGroups/dahatakemlus/providers/Microsoft.MachineLearningServices/workspaces/dahatakemlus/experiments/keras-mnist/runs/keras-mnist_1570404439206766\n\nStreaming azureml-logs/hyperdrive.txt\n=====================================\n\n\"<START>[2019-10-06T23:27:19.487762][API][INFO]Experiment created<END>\\n\"\"<START>[2019-10-06T23:27:20.470608][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space<END>\\n\"<START>[2019-10-06T23:27:20.7852049Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.<END>\"<START>[2019-10-06T23:27:20.683690][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.<END>\\n\"\n\nExecution Summary\n=================\nRunId: keras-mnist_1570404439206766\nWeb View: https://mlworkspace.azure.ai/portal/subscriptions/ca28e367-312d-4b07-a3fd-650390469a4e/resourceGroups/dahatakemlus/providers/Microsoft.MachineLearningServices/workspaces/dahatakemlus/experiments/keras-mnist/runs/keras-mnist_1570404439206766\n\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "{'runId': 'keras-mnist_1570404439206766',\n 'target': 'gpu-cluster',\n 'status': 'Completed',\n 'startTimeUtc': '2019-10-06T23:27:19.36587Z',\n 'endTimeUtc': '2019-10-06T23:47:58.727381Z',\n 'properties': {'primary_metric_config': '{\"name\": \"Accuracy\", \"goal\": \"maximize\"}',\n  'runTemplate': 'HyperDrive',\n  'azureml.runsource': 'hyperdrive',\n  'platform': 'AML',\n  'baggage': 'eyJvaWQiOiAiYTE3OTgxZTItYzQxNi00ZWM2LTkzMjQtYWJhNTIxYTJlMThiIiwgInRpZCI6ICI3MmY5ODhiZi04NmYxLTQxYWYtOTFhYi0yZDdjZDAxMWRiNDciLCAidW5hbWUiOiAiMGQ5NzM4MzAtMTM1ZC00ZmZhLTkyY2QtNGM1NzY1MGRjMjIwIn0',\n  'ContentSnapshotId': 'a6a44e2b-e3cb-4c8c-8063-a2e77c1cce47'},\n 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://dahatakestorage3a55ee0c9.blob.core.windows.net/azureml/ExperimentRun/dcid.keras-mnist_1570404439206766/azureml-logs/hyperdrive.txt?sv=2018-11-09&sr=b&sig=apyh28HgkjA5IaTtWKcL0xgtRCLwgn4OjDoyGaB5n6M%3D&st=2019-10-06T23%3A37%3A59Z&se=2019-10-07T07%3A47%3A59Z&sp=r'}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Find and register best model\nWhen all the jobs finish, we can find out the one that has the highest accuracy."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run = hdr.get_best_run_by_primary_metric()\nprint(best_run.get_details()['runDefinition']['arguments'])",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['--data-folder', '$AZUREML_DATAREFERENCE_bee59a4ff1314bfeb99c71968fcb5908', '--loss', 'categorical_crossentropy', '--gpu', '1', '--auto_mixed_precision', '1', '--batch-size', '50', '--epoch', '35', '--neurons-1', '128', '--neurons-2', '16', '--neurons-3', '64', '--kernel-size-1', '4', '--kernel-size-2', '7', '--pool-size', '3', '--learning-rate', '0.0511994336836241', '--activation', 'softsign', '--optimizer', 'SGD', '--dropout-1', '0.188002257850059', '--dropout-2', '0.315650241721424']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can then register the folder (and all files in it) as a model named `keras-dnn-mnist` under the workspace for deployment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_model = best_run.register_model(model_name='keras-mlp-mnist', model_path='outputs/model')",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Convert model file to ONNX\n\nreference: https://github.com/onnx/keras-onnx\n\nFirst, Download model file to Disk"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for f in best_run.get_file_names():\n    if f.startswith('outputs/model'):\n        output_file_path = os.path.join('./model/best', f.split('/')[-1])\n        print('Downloading from {} to {} ...'.format(f, output_file_path))\n        run.download_file(name=f, output_file_path=output_file_path)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Downloading from outputs/model/mnist.h5 to ./model/best/mnist.h5 ...\nDownloading from outputs/model/mnist.json to ./model/best/mnist.json ...\nDownloading from outputs/model/mnist_weights.h5 to ./model/best/mnist_weights.h5 ...\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Then, load model file from disk."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = load_model('model/best/mnist.h5')\nconvert_onnx(model, 'model/best/mnist.onnx')\nprint('onnx file saved')",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "name: \"conv2d_1_input\"\ntype {\n  tensor_type {\n    elem_type: 1\n    shape {\n      dim {\n        dim_value: 1\n        denotation: \"DATA_BATCH\"\n      }\n      dim {\n        dim_value: 1\n        denotation: \"DATA_CHANNEL\"\n      }\n      dim {\n        dim_value: 28\n        denotation: \"DATA_FEATURE\"\n      }\n      dim {\n        dim_value: 28\n        denotation: \"DATA_FEATURE\"\n      }\n    }\n  }\n  denotation: \"IMAGE\"\n}\n\nonnx file saved\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "maxluk"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "msauthor": "maxluk",
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}